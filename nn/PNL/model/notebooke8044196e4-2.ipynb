{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14076014,"sourceType":"datasetVersion","datasetId":8960391}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:04:59.623517Z","iopub.execute_input":"2026-01-02T12:04:59.624259Z","iopub.status.idle":"2026-01-02T12:04:59.650948Z","shell.execute_reply.started":"2026-01-02T12:04:59.624220Z","shell.execute_reply":"2026-01-02T12:04:59.649878Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/msum-sum/es/val.txt.src\n/kaggle/input/msum-sum/es/test.txt.urls\n/kaggle/input/msum-sum/es/test.txt.src\n/kaggle/input/msum-sum/es/test.txt.tgt\n/kaggle/input/msum-sum/es/val.txt.urls\n/kaggle/input/msum-sum/es/val.txt.tgt\n/kaggle/input/msum-sum/es/train.txt.src\n/kaggle/input/msum-sum/es/train.txt.tgt\n/kaggle/input/msum-sum/es/train.txt.urls\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T01:38:18.046985Z","iopub.execute_input":"2026-01-02T01:38:18.047298Z","iopub.status.idle":"2026-01-02T01:38:18.181147Z","shell.execute_reply.started":"2026-01-02T01:38:18.047273Z","shell.execute_reply":"2026-01-02T01:38:18.180192Z"}},"outputs":[{"name":"stdout","text":"kaggle\t     test.txt.src.tokenized   val.txt.src.tokenized\noutputs.zip  test.txt.tgt.tokenized   val.txt.tgt.tokenized\nsaved\t     train.txt.src.tokenized\nstate.db     train.txt.tgt.tokenized\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport re \nimport json\nfrom collections import Counter\nfrom tqdm import tqdm\nfrom typing import List, Tuple, Dict\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport time\nimport re\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T01:38:18.182189Z","iopub.execute_input":"2026-01-02T01:38:18.182492Z","iopub.status.idle":"2026-01-02T01:38:18.187745Z","shell.execute_reply.started":"2026-01-02T01:38:18.182458Z","shell.execute_reply":"2026-01-02T01:38:18.187017Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Constantes","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport time\nimport os\n\n# =================================================\n# DATA / VOCABULARY\n# =================================================\n# Rutas relativas al directorio del proyecto\nBASE_DIR = '/kaggle/input/msum-sum/'\nDATA_DIR = os.path.join(BASE_DIR, \"es\")\nVOCAB_NAME = \"Vocabulary.json\"\nCHECKPOINT_VOCABULARY_DIR = os.path.join(\"saved\", \"working\")\n\n# Ruta a embeddings pre-entrenados \nEMBEDDING_PATH = os.path.join('kaggle/working', \"wiki.es.vec\")\n\nMAX_VOCAB_SIZE = 50000\nMAX_LEN_SRC = 500\nMAX_LEN_TGT = 50\nBATCH_SIZE = 16\n\nPAD_TOKEN = \"[PAD]\"\nUNK_TOKEN = \"[UNK]\"\nSTART_DECODING = \"[START]\"\nEND_DECODING = \"[END]\"\n\nCREATE_VOCABULARY = not os.path.exists(\n    os.path.join(CHECKPOINT_VOCABULARY_DIR, VOCAB_NAME)\n)\n\n# =================================================\n# MODEL ARCHITECTURE\n# =================================================\nEMBEDDING_SIZE = 300 \nHIDDEN_SIZE = 256\n\nNUM_ENC_LAYERS = 1\nNUM_DEC_LAYERS = 1\nBIDIRECTIONAL = True\n \n\nIS_ATTENTION = True\nIS_PGEN = True\nIS_COVERAGE = True\nCOV_LOSS_LAMBDA = 1.0 \n\n# =================================================\n# DECODING\n# =================================================\nDECODING_STRATEGY = \"beam_search\"\nBEAM_SIZE = 5\n\n\nEPOCHS = 15\nWARMUP_EPOCHS = 0\nITERS_PER_EPOCH = None  \n\n\n\nLEARNER = \"adam\"\n\nLEARNING_RATE = 0.001 \nGRAD_CLIP = 2.0  \nTRAIN_BATCH_SIZE = 64  \nEVAL_BATCH_SIZE = 64\nSAVE_HISTORY = True\nSAVE_MODEL_EPOCH = True\nDROPOUT_RATIO = 0.3\n\n\n# =================================================\n# GPU / REPRODUCIBILITY\n# =================================================\nUSE_GPU = True\nGPU_ID = 0\nDEVICE = torch.device(f\"cuda:{GPU_ID}\" if USE_GPU and torch.cuda.is_available() else \"cpu\")\n\nSEED = 42\nREPRODUCIBILITY = False\n\n# =================================================\n# PATHS / LOGGING\n# =================================================\nCHECKPOINT_DIR = os.path.join('kaggle/working', \"saved\")\nGENERATED_TEXT_DIR = os.path.join('kaggle/working/', \"generated\")\nPLOT = False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T01:38:18.188600Z","iopub.execute_input":"2026-01-02T01:38:18.189180Z","iopub.status.idle":"2026-01-02T01:38:18.207061Z","shell.execute_reply.started":"2026-01-02T01:38:18.189162Z","shell.execute_reply":"2026-01-02T01:38:18.206410Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Configuración","metadata":{}},{"cell_type":"code","source":"class Config:\n    def __init__(\n        self,\n        **kwargs\n    ):\n        self.config_dict = {\n            **kwargs\n        }\n\n        self._init_device()\n        \n    def _init_device(self):\n        if self.config_dict[\"use_gpu\"] and torch.cuda.is_available():\n            self.config_dict[\"device\"] = torch.device(\n                f\"cuda:{self.config_dict['gpu_id']}\"\n            )\n        else:\n            self.config_dict[\"device\"] = torch.device(\"cpu\")\n\n    def __getitem__(self, item):\n        return self.config_dict.get(item, None)\n    \n    def get_config_dict(self):\n        return self.config_dict\n        \n    def __str__(self):\n        args_info = \"\\nHyper Parameters:\\n\"\n        for key, value in self.config_dict.items():\n            args_info += f\"{key}={value}\\n\"\n        return args_info\n\n    def __repr__(self):\n        return self.__str__()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T01:38:18.207908Z","iopub.execute_input":"2026-01-02T01:38:18.208176Z","iopub.status.idle":"2026-01-02T01:38:18.228966Z","shell.execute_reply.started":"2026-01-02T01:38:18.208152Z","shell.execute_reply":"2026-01-02T01:38:18.227471Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os \nfrom collections import Counter\nimport json\nimport re\nimport spacy\n\nclass Vocabulary:\n    def __init__(self, CREATE_VOCABULARY,\n                 PAD_TOKEN, UNK_TOKEN,\n                  END_DECODING, START_DECODING,\n                 MAX_VOCAB_SIZE, CHECKPOINT_VOCABULARY_DIR, DATA_DIR,VOCAB_NAME):\n        \n        self.vocab_name = VOCAB_NAME\n        self.create_vocabulary = CREATE_VOCABULARY\n        self.checkpoint_vocab_dir = CHECKPOINT_VOCABULARY_DIR\n        self.data_dir = DATA_DIR\n        self.max_vocab_size = MAX_VOCAB_SIZE\n        self._c = 0\n        try:\n            spacy.prefer_gpu()\n            self.nlp = spacy.load(\"es_core_news_sm\", disable=['parser','ner','lemmatizer','morphologizer','attribute_ruler'])\n            self.nlp.add_pipe('sentencizer')\n        except OSError:\n            print(\"Descargando modelo de spaCy español (Large)...\")\n            spacy.cli.download(\"es_core_news_sm\")\n            spacy.prefer_gpu()\n            self.nlp = spacy.load(\"es_core_news_sm\", disable=['parser','ner','lemmatizer','attribute_ruler','morphologizer'])\n            self.nlp.add_pipe('sentencizer')\n        # Token de Relleno (Padding) - Usado para igualar longitudes de secuencias.\n        self.pad_token = PAD_TOKEN\n        # Token Desconocido (Unknown) - Usado para palabras no vistas en el vocabulario.\n        self.unk_token = UNK_TOKEN\n        \n        # Tokens para Delimitación de Sentencias/Secuencias\n        self.start_decoding = START_DECODING\n        self.end_decoding = END_DECODING\n        \n   \n        # Diccionario para mapear palabras a sus IDs (índices)     \n        self.word_to_id = {}\n        # Lista para mapear IDs a sus palabras\n        self.id_to_word = []\n        # Contador de frecuencia de palabras\n        self.word_count = {}\n        \n        self._add_special_tokens()\n        \n    def total_size(self):\n        return len(self.word_to_id)\n\n    def word2id(self, word):\n        \"\"\"Retorna el id de la palabra o [UNK] id si es OOV.\"\"\"\n        if word not in self.word_to_id:\n          return self.word_to_id[self.unk_token]\n        return self.word_to_id[word]\n\n    def id2word(self, word_id):\n        \"\"\"Retorna la palabra dado el id si existe en el vocabulario\"\"\"\n        if 0 <= word_id < len(self.id_to_word):\n            return self.id_to_word[word_id]\n        \n        raise ValueError('Id no esta en el vocab: %d' % word_id)\n        \n    def _add_special_tokens(self):\n        \"\"\"Añade los tokens especiales al vocabulario.\"\"\"\n        # Se añaden en un orden específico para que sus IDs sean fijos.\n        special_tokens = [\n            self.pad_token, self.unk_token, \n            self.start_decoding, self.end_decoding\n        ]\n        \n        for token in special_tokens: #{'[PAD]':0,'[UNK]':1,'[START]':2,'[END]':3}\n            if token not in self.word_to_id:\n                self.word_to_id[token] = len(self.id_to_word)\n                self.id_to_word.append(token)\n                self.word_count[token] = 0 # Frecuencia inicial 0\n        \n        self.num_special_tokens = len(self.id_to_word)\n\n    def _load_vocabulary(self):\n        \"\"\"\n        Carga el vocabulario completo desde disco y restaura el estado interno.\n        \"\"\"\n        try:\n            vocab_path = os.path.join(self.checkpoint_vocab_dir, self.vocab_name)\n    \n            if not os.path.exists(vocab_path):\n                raise FileNotFoundError(f\"Vocabulario no encontrado en {vocab_path}\")\n            print(vocab_path)\n            with open(vocab_path, 'r', encoding='utf-8') as f:\n                saved_data = json.load(f)\n            # -------------------------\n            # Restaurar vocabulario base\n            # -------------------------\n                self.word_to_id = saved_data['word_to_id']\n                self.id_to_word = saved_data['id_to_word']\n                self.word_count = saved_data['word_count']\n                self._c = saved_data['size']\n        \n                # -------------------------\n                # Restaurar tokens especiales\n                # -------------------------\n                special_tokens = saved_data['special_tokens']\n    \n                self.pad_token = special_tokens['PAD']\n                self.unk_token = special_tokens['UNK']\n                self.start_decoding = special_tokens.get('START')\n                self.end_decoding = special_tokens.get('END_DECODING')\n        \n                self.num_special_tokens = saved_data['metadata']['num_special_tokens']\n        \n                # -------------------------\n                # Restaurar metadata\n                # -------------------------\n                metadata = saved_data['metadata']\n        \n                self.max_vocab_size = metadata['max_vocab_size']\n                self.data_dir = metadata['data_dir']\n                self.create_vocabulary = metadata['create_vocabulary']\n                self.vocab_name = metadata['vocab_name']\n                self.checkpoint_vocab_dir = metadata['checkpoint_dir']\n    \n            \n            if len(self.word_to_id) != len(self.id_to_word):\n                raise ValueError(\"Inconsistencia: word_to_id e id_to_word tienen tamaños distintos\")\n    \n            if self.pad_token not in self.word_to_id:\n                raise ValueError(\"Token PAD no encontrado en el vocabulario\")\n    \n            print(f\" Vocabulario cargado desde: {vocab_path}\")\n            print(f\" Tamaño total: {len(self.word_to_id)}\")\n            print(f\" Tokens especiales: {self.num_special_tokens}\")\n            print(f\" Tokens regulares: {self._c}\")\n    \n            return True\n    \n        except Exception as e:\n            print(f\"✗ Error cargando vocabulario: {e}\")\n            return False\n\n            \n    def size(self):\n        \"\"\"Retorna el tamaño real de el vocabulario\"\"\"\n        return self._c    \n        \n    def _clean_text(self, text,for_vocab=False):\n        \"\"\"Limpieza inicial de texto antes de pasar por spaCy.\"\"\"\n        # 1. Quitar HTML\n        text = re.sub(r'<[^>]+>', ' ', text)\n        \n        # Atrapa http, https, www y los que empiezan con //\n        url_pattern = r'(http[s]?://|www\\.|//)[^\\s/$.?#].[^\\s]*'\n        text = re.sub(url_pattern, ' ', text)\n        # 3. Limpieza de caracteres especiales y ruido\n        text = text.replace('\\xa0', ' ')\n        # Caracteres decorativos repetidos\n        text = re.sub(r'[~*\\-_=]{2,}', ' ', text)\n\n        if for_vocab:\n            # Quita números aislados: \"1\", \"2025\", \"10.5\", \"50%\"\n            # Y también combinaciones numéricas con guión: \"1-0\", \"24-7\", \"2023-2024\"\n            text = re.sub(r'\\b\\d+([.,-]\\d+)*%?\\b', ' ', text)\n        \n        text = text.replace('...', ' ')\n        \n        # 4. Normalizar espacios \n        return re.sub(r'\\s+', ' ', text).strip()\n\n    def _tokens_from_doc(self, doc, for_vocab=False):\n        \"\"\"Extrae y filtra tokens de un doc de spaCy.\"\"\"\n        tokens = []\n        for token in doc:\n            # Si estamos filtrando para el VOCABULARIO\n            if for_vocab:\n                # Omitimos Números y Fechas en el vocabulario fijo\n                if token.like_num or token.pos_ == \"NUM\":\n                    continue\n                # Intento de detectar fechas por forma básica\n                if re.match(r'\\d+[/-]\\d+', token.text):\n                    continue\n            \n            # Filtros comunes (Puntuación ruidosa, brackets, quotes)\n            if token.is_punct and token.text not in ['.', ',', '!', '?','¿']:\n                continue\n            if token.is_bracket or token.is_quote:\n                continue\n            \n            t = token.text\n            t = t.replace('``', '\"').replace(\"''\", '\"')\n            if t:\n                tokens.append(t)\n        return tokens\n\n    def process_text(self, text):\n        \"\"\"Procesa un único texto para el modelo (mantiene fechas/números).\"\"\"\n        text = self._clean_text(text,for_vocab=False)\n        doc = self.nlp(text)\n        return self._tokens_from_doc(doc, for_vocab=False)\n\n    \n    def _save_vocabulary(self):\n        \"\"\"Guarda el vocabulario completo en el disco.\"\"\"\n        try:\n            # Crear directorio si no existe\n            os.makedirs(self.checkpoint_vocab_dir, exist_ok=True)\n            \n            path = os.path.join(self.checkpoint_vocab_dir, self.vocab_name)\n            \n            # Preparar datos para guardar\n            save_data = {\n                'word_to_id': self.word_to_id,\n                'id_to_word': self.id_to_word,\n                'word_count': self.word_count,\n                'size': self._c,\n                'special_tokens': {\n                    'PAD': self.pad_token,\n                    'UNK': self.unk_token,\n                    'START': self.start_decoding,\n                    'END_DECODING': self.end_decoding\n                },\n                'metadata': {\n                    'max_vocab_size': self.max_vocab_size,\n                    'data_dir': self.data_dir,\n                    'create_vocabulary': self.create_vocabulary,\n                    'vocab_name': self.vocab_name,\n                    'checkpoint_dir': self.checkpoint_vocab_dir,\n                    'num_special_tokens': self.num_special_tokens,\n                    'total_size': len(self.word_to_id)\n                }\n            }\n            \n            # Guardar como JSON\n            with open(path, 'w', encoding='utf-8') as f:\n                json.dump(save_data, f, ensure_ascii=False, indent=4)\n            \n            print(f\"  Vocabulario guardado en: {path}\")\n            print(f\"  Tamaño total: {len(self.word_to_id)} palabras\")\n            print(f\"  Tokens especiales: {self.num_special_tokens}\")\n            print(f\"  Tokens regulares: {self._c}\")\n                       \n            return True\n            \n        except Exception as e:\n            print(f\"✗ Error al guardar el vocabulario: {e}\")\n            raise\n            \n    def _create_vocabulary(self):\n        import multiprocessing\n        num_cores = max(1, multiprocessing.cpu_count() - 2)\n        print(f\"Construyendo vocabulario usando {num_cores} núcleos...\")\n        print(f\"Construyendo vocabulario a partir de los datos en: {self.data_dir}\")\n        src_files = [os.path.join(self.data_dir, f\"{split}.txt.src\") for split in [\"train\"]]\n        tgt_files = [os.path.join(self.data_dir, f\"{split}.txt.tgt\") for split in [\"train\"]]\n        all_files = src_files + tgt_files\n        all_words = []\n       \n        word_counts = Counter()\n       \n        for file_path in all_files:\n            \n            def line_generator(path):\n                with open(path, \"r\", encoding=\"utf-8\") as f:\n                    for line in f:\n                        # Aplicamos la limpieza básica de strings antes de spaCy\n                        yield self._clean_text(line, for_vocab=True)\n                        \n            doc_stream = self.nlp.pipe(\n                line_generator(file_path), \n                batch_size=500,\n                num_workers=num_cores\n            )\n            for doc in tqdm(doc_stream, desc=f\"Procesando {os.path.basename(file_path)}\"):\n                tokens = self._tokens_from_doc(doc, for_vocab=True)\n                word_counts.update(tokens)\n      \n        # Calcular cuántas palabras regulares podemos añadir:\n        if self.max_vocab_size <= self.num_special_tokens:\n            raise ValueError(\n                f\"ERROR: MAX_VOCAB_SIZE ({self.max_vocab_size}) debe ser mayor que \"\n                f\"el número de tokens especiales ({self.num_special_tokens}). \"\n                \"Vocabulario muy pequeño.\"\n            )\n        limit = self.max_vocab_size - self.num_special_tokens\n        # Seleccionar las 'limit' palabras más comunes, excluyendo las que ya son tokens especiales\n        for word, count in word_counts.most_common(limit):\n            if word not in self.word_to_id and len(self.word_to_id) < self.max_vocab_size:\n                self.word_to_id[word] = len(self.id_to_word)\n                self.id_to_word.append(word)\n                self.word_count[word] = count\n                self._c+=1\n                \n        # Guardar el vocabulario \n        self._save_vocabulary()\n\n        print(f\"Vocabulario construido. Tamaño final: {len(self.word_to_id)}\")\n        return True\n        \n    def build_vocabulary(self):\n        if not self.create_vocabulary:\n            return self._load_vocabulary()\n        return self._create_vocabulary()\n\n    def load_pretrained_embeddings(self, embedding_path, embedding_dim):\n        \"\"\"\n        Carga embeddings pre-entrenados y los alinea con el vocabulario actual.\n        Solo realiza coincidencias EXACTAS. Las palabras no encontradas (incluyendo\n        variaciones de mayúsculas/minúsculas no presentes en el archivo) serán\n        aprendidas por el modelo durante el entrenamiento.\n        \"\"\"\n        import torch\n        import numpy as np\n        from tqdm import tqdm\n\n        # 1. Verificar si el archivo existe, si no, descargar SBW (News) por defecto\n        if embedding_path is not None and not os.path.exists(embedding_path):\n            print(f\"⚠ Archivo {embedding_path} no encontrado.\")\n            if \"sbw_news.vec\" in embedding_path:\n                print(\"Iniciando descarga automática de SBW News Embeddings (Noticias en español)...\")\n                self.download_spanish_embeddings(os.path.dirname(embedding_path), type='sbw')\n            elif \"wiki.es.vec\" in embedding_path:\n                print(\"Iniciando descarga automática de FastText Spanish...\")\n                self.download_spanish_embeddings(os.path.dirname(embedding_path), type='fasttext')\n            else:\n                print(\"Usando inicialización aleatoria.\")\n                return torch.randn(len(self.id_to_word), embedding_dim) * 0.1\n\n        vocab_size = len(self.id_to_word)\n        # Inicialización aleatoria para que el modelo \"aprenda\" lo que no esté en los embeddings\n        weights = torch.randn(vocab_size, embedding_dim) * 0.1\n        \n        if embedding_path is None:\n            return weights\n\n        print(f\"Cargando embeddings (Solo Coincidencias Exactas) desde {embedding_path}...\")\n        \n        found_indices = set()\n        \n        try:\n            with open(embedding_path, 'r', encoding='utf-8', errors='ignore') as f:\n                header = f.readline().split()\n                if len(header) != 2:\n                    f.seek(0)\n                \n                for line in tqdm(f, desc=\"Alineando embeddings\"):\n                    parts = line.rstrip().split(' ')\n                    if len(parts) < embedding_dim + 1:\n                        continue\n                        \n                    emb_word = parts[0]\n                    \n                    # Búsqueda Exacta ÚNICAMENTE\n                    if emb_word in self.word_to_id:\n                        idx = self.word_to_id[emb_word]\n                        if idx not in found_indices:\n                            vec = np.array([float(x) for x in parts[1:embedding_dim+1]])\n                            weights[idx] = torch.from_numpy(vec)\n                            found_indices.add(idx)\n                                \n            print(f\"✓ Cobertura Exacta: {len(found_indices)} / {vocab_size} ({len(found_indices)/vocab_size*100:.1f}%)\")\n            print(f\"  - Las {vocab_size - len(found_indices)} palabras restantes serán aprendidas desde cero.\")\n\n            if self.pad_token in self.word_to_id:\n                weights[self.word_to_id[self.pad_token]] = torch.zeros(embedding_dim)\n                \n        except Exception as e:\n            print(f\"✗ Error cargando embeddings: {e}\")\n            \n        return weights\n\n    def download_spanish_embeddings(self, target_dir, type='sbw'):\n        \"\"\"\n        Descarga y extrae vectores pre-entrenados.\n        type: 'fasttext' (Wikipedia/CC) o 'sbw' (Spanish Billion Words - Noticias/Libros)\n        \"\"\"\n        import urllib.request\n        import os\n        import gzip\n        import shutil\n\n        os.makedirs(target_dir, exist_ok=True)\n        \n        if type == 'sbw':\n            # Spanish Billion Words (Más orientado a Noticias/Libros)\n            url = \"http://dcc.uchile.cl/~jperez/word-embeddings/glove-sbwc.i2e.300d.vec.gz\"\n            target_name = \"sbw_news.vec\"\n        else:\n            # FastText CC Spanish\n            url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\"\n            target_name = \"wiki.es.vec\"\n\n        gz_path = os.path.join(target_dir, f\"{target_name}.gz\")\n        vec_path = os.path.join(target_dir, target_name)\n        \n        print(f\"Descargando embeddings de tipo '{type}' desde {url}...\")\n        try:\n            urllib.request.urlretrieve(url, gz_path)\n            print(f\"✓ Descarga completada. Descomprimiendo en {vec_path}...\")\n            \n            with gzip.open(gz_path, 'rb') as f_in:\n                with open(vec_path, 'wb') as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n            \n            print(f\"✓ Extracción completada.\")\n            os.remove(gz_path) \n            return vec_path\n        except Exception as e:\n            print(f\"✗ Error descargando: {e}\")\n            return None\n        ","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2026-01-02T01:38:18.231042Z","iopub.execute_input":"2026-01-02T01:38:18.231333Z","iopub.status.idle":"2026-01-02T01:38:18.286475Z","shell.execute_reply.started":"2026-01-02T01:38:18.231311Z","shell.execute_reply":"2026-01-02T01:38:18.285722Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\"\"\"import os\nimport sys\nfrom tqdm import tqdm\nimport multiprocessing\n\n\n\ndef preprocess_files(vocab, files, is_source=True):\n    num_cores = max(1, multiprocessing.cpu_count() - 2)\n    \n    for file_path, original_name in files:\n        output_path = original_name + \".tokenized\"\n                    \n        print(f\"Procesando {os.path.basename(file_path)} -> {os.path.basename(output_path)}\")\n        \n        def line_generator(path):\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    yield vocab._clean_text(line, for_vocab=False)\n        \n        doc_stream = vocab.nlp.pipe(\n            line_generator(file_path), \n            batch_size=1000, \n        )\n        \n        with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n            for doc in tqdm(doc_stream, desc=f\"  Tokenizando...\"):\n                # Procesamos por oraciones para mantener la estructura\n                sentences_tokens = []\n                for sent in doc.sents:\n                    tokens = vocab._tokens_from_doc(sent, for_vocab=False)\n                    if tokens:\n                        sentences_tokens.append(\" \".join(tokens))\n                \n                # Unimos las oraciones con \"[.]\" para que dataset.py las recupere de forma segura\n                f_out.write(\" [.] \".join(sentences_tokens) + \"\\n\")\n\ndef main():\n    print(\"=== INICIANDO PREPROCESAMIENTO ===\")\n    \n    # 1. Cargar Vocabulario\n    print(\"Cargando instancia de Vocabulary...\")\n    vocab = Vocabulary(\n        CREATE_VOCABULARY=False,\n        PAD_TOKEN=PAD_TOKEN,\n        UNK_TOKEN=UNK_TOKEN,\n        START_DECODING=START_DECODING,\n        END_DECODING=END_DECODING,\n        MAX_VOCAB_SIZE=MAX_VOCAB_SIZE,\n        CHECKPOINT_VOCABULARY_DIR=CHECKPOINT_VOCABULARY_DIR,\n        DATA_DIR=DATA_DIR,\n        VOCAB_NAME=VOCAB_NAME\n    )\n\n    # 2. Definir archivos\n    splits = ['train', 'val', 'test']\n    src_files = []\n    tgt_files = []\n    \n    for split in splits:\n        s_name = f\"{split}.txt.src\"\n        t_name = f\"{split}.txt.tgt\"\n        s_path = os.path.join(DATA_DIR, s_name)\n        t_path = os.path.join(DATA_DIR, t_name)\n        \n        if os.path.exists(s_path):\n            src_files.append((s_path, s_name))\n        if os.path.exists(t_path):\n            tgt_files.append((t_path, t_name))\n            \n    # 3. Procesar\n    if src_files:\n        print(f\"\\nArchivos Source encontrados: {len(src_files)}\")\n        preprocess_files(vocab, src_files, is_source=True)\n    \n    if tgt_files:\n        print(f\"\\nArchivos Target encontrados: {len(tgt_files)}\")\n        preprocess_files(vocab, tgt_files, is_source=False)\n    \n    print(\"\\n=== PREPROCESAMIENTO COMPLETADO ===\")\n\nif __name__ == \"__main__\":\n    main()\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T01:38:18.287362Z","iopub.execute_input":"2026-01-02T01:38:18.287623Z","iopub.status.idle":"2026-01-02T01:38:18.306254Z","shell.execute_reply.started":"2026-01-02T01:38:18.287601Z","shell.execute_reply":"2026-01-02T01:38:18.305537Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'import os\\nimport sys\\nfrom tqdm import tqdm\\nimport multiprocessing\\n\\n\\n\\ndef preprocess_files(vocab, files, is_source=True):\\n    num_cores = max(1, multiprocessing.cpu_count() - 2)\\n    \\n    for file_path, original_name in files:\\n        output_path = original_name + \".tokenized\"\\n                    \\n        print(f\"Procesando {os.path.basename(file_path)} -> {os.path.basename(output_path)}\")\\n        \\n        def line_generator(path):\\n            with open(path, \"r\", encoding=\"utf-8\") as f:\\n                for line in f:\\n                    yield vocab._clean_text(line, for_vocab=False)\\n        \\n        doc_stream = vocab.nlp.pipe(\\n            line_generator(file_path), \\n            batch_size=1000, \\n        )\\n        \\n        with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\\n            for doc in tqdm(doc_stream, desc=f\"  Tokenizando...\"):\\n                # Procesamos por oraciones para mantener la estructura\\n                sentences_tokens = []\\n                for sent in doc.sents:\\n                    tokens = vocab._tokens_from_doc(sent, for_vocab=False)\\n                    if tokens:\\n                        sentences_tokens.append(\" \".join(tokens))\\n                \\n                # Unimos las oraciones con \"[.]\" para que dataset.py las recupere de forma segura\\n                f_out.write(\" [.] \".join(sentences_tokens) + \"\\n\")\\n\\ndef main():\\n    print(\"=== INICIANDO PREPROCESAMIENTO ===\")\\n    \\n    # 1. Cargar Vocabulario\\n    print(\"Cargando instancia de Vocabulary...\")\\n    vocab = Vocabulary(\\n        CREATE_VOCABULARY=False,\\n        PAD_TOKEN=PAD_TOKEN,\\n        UNK_TOKEN=UNK_TOKEN,\\n        START_DECODING=START_DECODING,\\n        END_DECODING=END_DECODING,\\n        MAX_VOCAB_SIZE=MAX_VOCAB_SIZE,\\n        CHECKPOINT_VOCABULARY_DIR=CHECKPOINT_VOCABULARY_DIR,\\n        DATA_DIR=DATA_DIR,\\n        VOCAB_NAME=VOCAB_NAME\\n    )\\n\\n    # 2. Definir archivos\\n    splits = [\\'train\\', \\'val\\', \\'test\\']\\n    src_files = []\\n    tgt_files = []\\n    \\n    for split in splits:\\n        s_name = f\"{split}.txt.src\"\\n        t_name = f\"{split}.txt.tgt\"\\n        s_path = os.path.join(DATA_DIR, s_name)\\n        t_path = os.path.join(DATA_DIR, t_name)\\n        \\n        if os.path.exists(s_path):\\n            src_files.append((s_path, s_name))\\n        if os.path.exists(t_path):\\n            tgt_files.append((t_path, t_name))\\n            \\n    # 3. Procesar\\n    if src_files:\\n        print(f\"\\nArchivos Source encontrados: {len(src_files)}\")\\n        preprocess_files(vocab, src_files, is_source=True)\\n    \\n    if tgt_files:\\n        print(f\"\\nArchivos Target encontrados: {len(tgt_files)}\")\\n        preprocess_files(vocab, tgt_files, is_source=False)\\n    \\n    print(\"\\n=== PREPROCESAMIENTO COMPLETADO ===\")\\n\\nif __name__ == \"__main__\":\\n    main()'"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import os\nfrom typing import List, Tuple, Dict\nimport torch\nfrom torch.utils.data import Dataset\nclass PGNDataset(Dataset):\n    \"\"\"\n    Dataset para PGN con OOVs dinámicos y head truncation\n    \"\"\"\n    \n    def __init__(self, vocab, MAX_LEN_SRC: int, MAX_LEN_TGT: int, data_dir: str, split: str):\n        self.vocab = vocab\n        self.MAX_LEN_SRC = MAX_LEN_SRC\n        self.MAX_LEN_TGT = MAX_LEN_TGT\n        self.data_dir = data_dir\n        self.split = split\n\n        self.PAD_ID = self.vocab.word2id(self.vocab.pad_token)\n        self.SOS_ID = self.vocab.word2id(self.vocab.start_decoding)\n        self.EOS_ID = self.vocab.word2id(self.vocab.end_decoding)\n        self.UNK_ID = self.vocab.word2id(self.vocab.unk_token)\n\n        src_path = os.path.join(data_dir, f\"{split}.txt.src\")\n        tgt_path = os.path.join(data_dir, f\"{split}.txt.tgt\")\n        \n        # Verificar si existen versiones tokenizadas\n        src_tokenized_path = f\"{split}.txt.src\" + \".tokenized\"\n        tgt_tokenized_path = f\"{split}.txt.tgt\" + \".tokenized\"\n        \n        self.is_tokenized = False\n        \n        if os.path.exists(src_tokenized_path) and os.path.exists(tgt_tokenized_path):\n            print(f\"✓ Usando archivos TOKENIZADOS para {split} (Carga optimizada en Kaggle)\")\n            src_path = src_tokenized_path\n            tgt_path = tgt_tokenized_path\n            self.is_tokenized = True\n        else:\n            print(f\"⚠ Usando archivos RAW para {split} (Tokenización en tiempo real -> LENTO)\")\n\n        if not os.path.exists(src_path) or not os.path.exists(tgt_path):\n            raise FileNotFoundError(f\"Split '{split}' no encontrado\")\n\n        with open(src_path, encoding=\"utf-8\") as f:\n            self.src_lines = f.readlines()\n\n        with open(tgt_path, encoding=\"utf-8\") as f:\n            self.tgt_lines = f.readlines()\n\n        assert len(self.src_lines) == len(self.tgt_lines)\n    \n    def _get_extended_src_ids(\n        self, src_tokens_raw: List[str]\n    ) -> Tuple[List[int], int, Dict[str, int], List[str]]:\n        \"\"\"Obtener IDs extendidos para fuente con OOVs\"\"\"\n        extended_src_ids = []\n        temp_oov_map = {}\n        oov_words = []\n        \n        vocab_size = len(self.vocab.word_to_id)\n        oov_id_counter = vocab_size  # Empezar después del vocabulario base\n        \n        for token in src_tokens_raw:\n            base_id = self.vocab.word2id(token)\n            \n            if base_id == self.UNK_ID:\n                if token not in temp_oov_map:\n                    temp_oov_map[token] = oov_id_counter\n                    oov_words.append(token)\n                    oov_id_counter += 1\n                extended_src_ids.append(temp_oov_map[token])\n            else:\n                extended_src_ids.append(base_id)\n        \n        extended_vocab_size = oov_id_counter\n        return extended_src_ids, extended_vocab_size, temp_oov_map, oov_words\n    \n    def _map_target_to_extended_ids(self, tgt_tokens, oov_map):\n        \"\"\"Mapear tokens objetivo a IDs extendidos\"\"\"\n        mapped_ids = []\n        for token in tgt_tokens:\n            base_id = self.vocab.word2id(token)\n            if base_id == self.UNK_ID and token in oov_map:\n                mapped_ids.append(oov_map[token])\n            else:\n                mapped_ids.append(base_id)\n        return mapped_ids\n    \n    def _pad_sequence(self, ids, max_len):\n        \"\"\"Rellenar secuencia con PAD_ID\"\"\"\n        if len(ids) < max_len:\n            ids.extend([self.PAD_ID] * (max_len - len(ids)))\n        return ids[:max_len]\n    \n    def __len__(self):\n        return len(self.src_lines)\n    \n    def __getitem__(self, idx):\n        src_line = self.src_lines[idx].strip()\n        tgt_line = self.tgt_lines[idx].strip()\n                         \n        # --- 1. Head truncation por oraciones ---\n        if self.is_tokenized:\n            # Si ya está tokenizado, recuperamos las oraciones separando por \"[.]\"\n            # ya que preprocess_data.py las guardó así expresamente.\n            raw_sentences = src_line.split(\" [.] \")\n        else:\n             # Usamos spaCy/NLTK para dividir oraciones (más lento pero robusto)\n             # Nota: Se usa NLTK según la última configuración aceptada\n             raw_sentences = nltk.sent_tokenize(src_line, language='spanish')\n             \n        trimmed_src_tokens = []\n        \n        for sentence in raw_sentences:\n            if self.is_tokenized:\n                # Fast path: Ya está tokenizado por palabras\n                sentence_tokens = sentence.split()\n                # El split por \" [.] \" quitó el punto, lo añadimos para el modelo\n                tokens_to_add = sentence_tokens \n            else:\n                # Slow path: Tokenización en tiempo real\n                sentence_tokens = self.vocab.process_text(sentence.strip())\n                if not sentence_tokens:\n                    continue\n                tokens_to_add = sentence_tokens\n            \n            if len(trimmed_src_tokens) + len(tokens_to_add) > self.MAX_LEN_SRC:\n                break\n            \n            trimmed_src_tokens.extend(tokens_to_add)\n        \n        # Eliminar posible punto final si la última oración ya lo tenía y lo añadimos doble (opcional)\n        # O simplemente dejar la lógica fluir.\n        \n        # --- 2. Encoder con OOVs ---\n        ext_src_ids, ext_vocab_size, oov_map, oov_words = \\\n            self._get_extended_src_ids(trimmed_src_tokens)\n        \n        max_oov_len = ext_vocab_size - len(self.vocab.word_to_id)\n        \n        # Extended encoder input (con IDs extendidos para pointer-generator)\n        # Dynamic Batching: NO rellenamos aquí, solo truncamos\n        # extended_encoder_input = self._pad_sequence(ext_src_ids.copy(), self.MAX_LEN_SRC)\n        extended_encoder_input = ext_src_ids[:self.MAX_LEN_SRC]\n        \n        # Encoder input regular (convertir OOVs a UNK para embeddings)\n        encoder_input = [\n            i if i < len(self.vocab.word_to_id) else self.UNK_ID\n            for i in extended_encoder_input\n        ]\n        \n        # --- 3. Decoder ---\n        if self.is_tokenized:\n            tgt_tokens = tgt_line.strip().split()\n        else:\n            tgt_tokens = self.vocab.process_text(tgt_line)\n            \n        tgt_ext_ids = self._map_target_to_extended_ids(tgt_tokens, oov_map)\n        \n        MAX_RAW_TGT_LEN = self.MAX_LEN_TGT - 1\n        tgt_ext_ids = tgt_ext_ids[:MAX_RAW_TGT_LEN]\n        \n        # Decoder input (convertir OOVs a UNK para embeddings)\n        decoder_input_ids = [self.SOS_ID]\n        for token_id in tgt_ext_ids:\n            if token_id < len(self.vocab.word_to_id):\n                decoder_input_ids.append(token_id)\n            else:\n                decoder_input_ids.append(self.UNK_ID)\n        \n        # Decoder target (mantener extended IDs para loss)\n        decoder_output_ids = tgt_ext_ids + [self.EOS_ID]\n        \n        # Dynamic Batching: NO rellenamos aquí\n        # decoder_input = self._pad_sequence(decoder_input_ids, self.MAX_LEN_TGT)\n        # decoder_output = self._pad_sequence(decoder_output_ids, self.MAX_LEN_TGT)\n        decoder_input = decoder_input_ids[:self.MAX_LEN_TGT]\n        decoder_output = decoder_output_ids[:self.MAX_LEN_TGT]\n        \n        # --- 4. Información adicional ---\n        encoder_length = len(trimmed_src_tokens)\n        # Mask será dinámica en collate, aquí solo devolvemos el largo real\n        # encoder_mask = [1] * encoder_length + [0] * (self.MAX_LEN_SRC - encoder_length)\n        encoder_mask = [1] * encoder_length\n        \n        return {\n            \"encoder_input\": torch.tensor(encoder_input, dtype=torch.long),\n            \"extended_encoder_input\": torch.tensor(extended_encoder_input, dtype=torch.long),\n            \"encoder_length\": torch.tensor(encoder_length, dtype=torch.long),\n            \"encoder_mask\": torch.tensor(encoder_mask, dtype=torch.bool),\n            \"decoder_input\": torch.tensor(decoder_input, dtype=torch.long),\n            \"decoder_target\": torch.tensor(decoder_output, dtype=torch.long),\n            \"max_oov_len\": max_oov_len,\n            \"oov_words\": oov_words,\n            \"pad_id\": self.PAD_ID  # Pasamos PAD_ID para el collate\n        }\n\ndef pgn_collate_fn(batch):\n    \"\"\"Función para combinar muestras en batches\"\"\"\n    # Filtrar ejemplos con encoder_length <= 0\n    filter_batch = []\n    for x in batch:\n        if x['encoder_length'].item() > 0:\n            filter_batch.append(x)\n    \n    # Si todos los ejemplos fueron filtrados, retornar None\n    if len(filter_batch) == 0:\n        return None\n    \n    batch = filter_batch\n    max_oov = max(x[\"max_oov_len\"] for x in batch)\n    \n    # 1. Obtener lengths máximos del batch actual (Dynamic Batching)\n    max_enc_len = max(x[\"encoder_length\"].item() for x in batch)\n    max_dec_len = max(len(x[\"decoder_input\"]) for x in batch)\n    \n    pad_id = batch[0][\"pad_id\"]\n    \n    def pad_tensor(t, length, val):\n        \"\"\"Pad tensor to length with val\"\"\"\n        return torch.cat([t, torch.full((length - len(t),), val, dtype=t.dtype)])\n\n    def pad_oov(words):\n        \"\"\"Rellenar lista de OOVs con strings vacíos\"\"\"\n        return words + [\"\"] * (max_oov - len(words))\n    \n    return {\n        # Encoders: Pad a max_enc_len\n        \"encoder_input\": torch.stack([pad_tensor(x[\"encoder_input\"], max_enc_len, pad_id) for x in batch]),\n        \"extended_encoder_input\": torch.stack([pad_tensor(x[\"extended_encoder_input\"], max_enc_len, pad_id) for x in batch]),\n        \"encoder_length\": torch.stack([x[\"encoder_length\"] for x in batch]),\n        \"encoder_mask\": torch.stack([pad_tensor(x[\"encoder_mask\"], max_enc_len, 0) for x in batch]), # 0 es False/Pad\n        \n        # Decoders: Pad a max_dec_len\n        \"decoder_input\": torch.stack([pad_tensor(x[\"decoder_input\"], max_dec_len, pad_id) for x in batch]),\n        \"decoder_target\": torch.stack([pad_tensor(x[\"decoder_target\"], max_dec_len, pad_id) for x in batch]),\n        \n        \"max_oov_len\": torch.tensor([x[\"max_oov_len\"] for x in batch], dtype=torch.long),\n        \"oov_words\": [pad_oov(x[\"oov_words\"]) for x in batch]\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T01:38:18.433145Z","iopub.execute_input":"2026-01-02T01:38:18.433484Z","iopub.status.idle":"2026-01-02T01:38:18.455098Z","shell.execute_reply.started":"2026-01-02T01:38:18.433459Z","shell.execute_reply":"2026-01-02T01:38:18.454339Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom typing import List, Tuple\nimport numpy as np\n\nclass Hypothesis:\n    \"\"\"\n    Representa una hipótesis durante beam search.\n    \"\"\"\n    def __init__(self, tokens, log_probs, decoder_state, context_vector, coverage, p_gens=None):\n        \"\"\"\n        Args:\n            tokens: List[int] - Secuencia de tokens generados\n            log_probs: List[float] - Log probabilities de cada token\n            decoder_state: Tuple (h, c) - Estado del decoder\n            context_vector: Tensor - Context vector actual\n            context_vector: Tensor - Context vector actual\n            coverage: Tensor - Coverage acumulado\n            p_gens: List[Tensor] - Lista de p_gen para cada paso\n        \"\"\"\n        self.tokens = tokens\n        self.log_probs = log_probs\n        self.decoder_state = decoder_state\n        self.context_vector = context_vector\n        self.coverage = coverage\n        self.p_gens = p_gens if p_gens is not None else []\n    \n    def extend(self, token, log_prob, decoder_state, context_vector, coverage, p_gen):\n        \"\"\"\n        Extiende la hipótesis con un nuevo token.\n        \n        Returns:\n            Nueva Hypothesis\n        \"\"\"\n        return Hypothesis(\n            tokens=self.tokens + [token],\n            log_probs=self.log_probs + [log_prob],\n            decoder_state=decoder_state,\n            context_vector=context_vector,\n            coverage=coverage,\n            p_gens=self.p_gens + [p_gen]\n        )\n    \n    @property\n    def avg_log_prob(self):\n        \"\"\"Promedio de log probabilities (para ranking).\"\"\"\n        return sum(self.log_probs) / len(self.tokens)\n    \n    @property\n    def latest_token(self):\n        \"\"\"Último token generado.\"\"\"\n        return self.tokens[-1]\n\n\nclass BeamSearch:\n    \"\"\"\n    Implementa Beam Search para decodificación.\n    \"\"\"\n    \n    def __init__(self, model, vocab, beam_size=4, max_len=50, min_len=10):\n        \"\"\"\n        Args:\n            model: PointerGeneratorNetwork\n            vocab: Vocabulary object\n            beam_size: Tamaño del beam\n            max_len: Longitud máxima de generación\n            min_len: Longitud mínima (penaliza secuencias cortas)\n        \"\"\"\n        self.model = model\n        self.vocab = vocab\n        self.beam_size = beam_size\n        self.max_len = max_len\n        self.min_len = min_len\n        \n        self.start_id = vocab.word2id(vocab.start_decoding)\n        self.end_id = vocab.word2id(vocab.end_decoding)\n        self.unk_id = vocab.word2id(vocab.unk_token)\n        self.vocab_size = len(vocab.word_to_id)\n        \n        self.device = model.device\n    \n    def search(self, batch):\n        \"\"\"\n        Realiza beam search para un batch (asume batch_size=1).\n        \n        Args:\n            batch: Dict con encoder inputs\n            \n        Returns:\n            best_hypothesis: Hypothesis con la mejor secuencia\n        \"\"\"\n        # Asegurar batch_size = 1\n        encoder_input = batch['encoder_input'].to(self.device)  # (1, src_len)\n        extended_encoder_input = batch['extended_encoder_input'].to(self.device)\n        encoder_length = batch['encoder_length'].to(self.device)\n        encoder_mask = batch['encoder_mask'].to(self.device)\n        \n        batch_size = encoder_input.size(0)\n        assert batch_size == 1, \"Beam search solo soporta batch_size=1\"\n        \n        src_len = encoder_input.size(1)\n        \n        # 1. Encoder\n        encoder_outputs, decoder_state = self.model.encoder(encoder_input, encoder_length)\n        # encoder_outputs: (1, src_len, hidden_size * 2)\n        \n        # 2. Inicializar beam\n        initial_context = torch.zeros(1, self.model.config['hidden_size'] * 2, device=self.device)\n        initial_coverage = torch.zeros(1, src_len, device=self.device) if self.model.is_coverage else None\n        \n        # Hipótesis inicial con START token\n        initial_hyp = Hypothesis(\n            tokens=[self.start_id],\n            log_probs=[0.0],\n            decoder_state=decoder_state,\n            context_vector=initial_context,\n            coverage=initial_coverage,\n            p_gens=[]\n        )\n        \n        hypotheses = [initial_hyp]  # Beam actual\n        completed = []  # Hipótesis completas (con END)\n        \n        # 3. Beam search loop\n        for step in range(self.max_len):\n            if len(completed) >= self.beam_size:\n                break\n            \n            all_candidates = []\n            \n            # Expandir cada hipótesis en el beam\n            for hyp in hypotheses:\n                # Si ya terminó, mover a completed\n                if hyp.latest_token == self.end_id:\n                    completed.append(hyp)\n                    continue\n                \n                # Preparar input para el decoder\n                decoder_input = torch.tensor(\n                    [hyp.latest_token],\n                    dtype=torch.long,\n                    device=self.device\n                )\n                \n                # Convertir OOV a UNK para embeddings\n                if decoder_input.item() >= self.vocab_size:\n                    decoder_input = torch.tensor(\n                        [self.unk_id],\n                        dtype=torch.long,\n                        device=self.device\n                    )\n                \n                # Decoder step\n                final_dist, new_decoder_state, new_context, attention_dist, p_gen, new_coverage = self.model.decoder(\n                    decoder_input=decoder_input,\n                    decoder_state=hyp.decoder_state,\n                    encoder_outputs=encoder_outputs,\n                    encoder_mask=encoder_mask,\n                    extended_encoder_input=extended_encoder_input,\n                    context_vector=hyp.context_vector,\n                    coverage=hyp.coverage\n                )\n                \n                # Log probabilities\n                log_probs = torch.log(final_dist + 1e-12)  # (1, extended_vocab_size)\n                log_probs = log_probs.squeeze(0)  # (extended_vocab_size,)\n                \n                # Penalizar UNK si queremos\n                log_probs[self.unk_id] -= 100.0\n                \n                # Penalizar END si estamos antes de min_len\n                if step < self.min_len:\n                    log_probs[self.end_id] = -1e20\n                \n                # Top-k candidatos\n                top_k_log_probs, top_k_ids = torch.topk(log_probs, self.beam_size * 2)\n                \n                # Crear nuevas hipótesis\n                for i in range(self.beam_size * 2):\n                    token_id = top_k_ids[i].item()\n                    token_log_prob = top_k_log_probs[i].item()\n                    \n                    new_hyp = hyp.extend(\n                        token=token_id,\n                        log_prob=token_log_prob,\n                        decoder_state=new_decoder_state,\n                        context_vector=new_context,\n                        coverage=new_coverage,\n                        p_gen=p_gen if p_gen is not None else torch.tensor([[1.0]], device=self.device)\n                    )\n                    \n                    all_candidates.append(new_hyp)\n            \n            # Ordenar candidatos por avg_log_prob\n            all_candidates.sort(key=lambda h: h.avg_log_prob, reverse=True)\n            \n            # Seleccionar top beam_size hipótesis\n            hypotheses = all_candidates[:self.beam_size]\n        \n        # 4. Si no hay hipótesis completas, tomar las mejores actuales\n        if len(completed) == 0:\n            completed = hypotheses\n        \n        # Ordenar por avg_log_prob y retornar la mejor\n        completed.sort(key=lambda h: h.avg_log_prob, reverse=True)\n        best_hypothesis = completed[0]\n        \n        return best_hypothesis\n    \n    def decode_batch(self, data_loader, num_examples=None):\n        \"\"\"\n        Decodifica múltiples ejemplos usando beam search.\n        \n        Args:\n            data_loader: DataLoader\n            num_examples: Número de ejemplos a decodificar (None = todos)\n            \n        Returns:\n            List[List[int]]: Secuencias generadas\n        \"\"\"\n        self.model.eval()\n        generated_sequences = []\n        \n        with torch.no_grad():\n            for i, batch in enumerate(data_loader):\n                if num_examples is not None and i >= num_examples:\n                    break\n                \n                # Beam search espera batch_size=1\n                # Si el batch tiene más de 1, procesar uno por uno\n                batch_size = batch['encoder_input'].size(0)\n                \n                for b in range(batch_size):\n                    # Extraer ejemplo individual\n                    single_batch = {\n                        'encoder_input': batch['encoder_input'][b:b+1],\n                        'extended_encoder_input': batch['extended_encoder_input'][b:b+1],\n                        'encoder_length': batch['encoder_length'][b:b+1],\n                        'encoder_mask': batch['encoder_mask'][b:b+1]\n                    }\n                    \n                    # Beam search\n                    best_hyp = self.search(single_batch)\n                    generated_sequences.append(best_hyp.tokens)\n                    \n                    if num_examples is not None and len(generated_sequences) >= num_examples:\n                        break\n        \n        return generated_sequences\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T01:38:18.456442Z","iopub.execute_input":"2026-01-02T01:38:18.456736Z","iopub.status.idle":"2026-01-02T01:38:18.483572Z","shell.execute_reply.started":"2026-01-02T01:38:18.456720Z","shell.execute_reply":"2026-01-02T01:38:18.482784Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport math\n\nclass ScheduledOptimizer:\n    \"\"\"\n    Optimizer con learning rate scheduling para Pointer-Generator Network.\n    Implementa:\n    - Warm-up lineal\n    - Decaimiento (opcional)\n    - Gradient clipping\n    \"\"\"\n    \n    def __init__(self, optimizer, config):\n        \"\"\"\n        Args:\n            optimizer: PyTorch optimizer (Adam, SGD, etc.)\n            config: Config object con hiperparámetros\n        \"\"\"\n        self.optimizer = optimizer\n        self.config = config\n        \n        self.initial_lr = config['learning_rate']\n        self.current_lr = config['learning_rate']\n        self.warmup_epochs = config['warmup_epochs'] if config['warmup_epochs'] else 0\n        self.grad_clip = config['grad_clip']\n        \n        self.current_epoch = 0\n        self.current_step = 0\n    \n    def step(self):\n        \"\"\"Realiza un paso de optimización con gradient clipping.\"\"\"\n        # Gradient clipping\n        if self.grad_clip > 0:\n            torch.nn.utils.clip_grad_norm_(\n                self._get_parameters(),\n                self.grad_clip\n            )\n        \n        self.optimizer.step()\n        self.current_step += 1\n    \n    def zero_grad(self):\n        \"\"\"Resetea los gradientes.\"\"\"\n        self.optimizer.zero_grad()\n    \n    def update_learning_rate(self, epoch):\n        \"\"\"\n        Actualiza el learning rate basado en el epoch actual.\n        \n        Args:\n            epoch: Epoch actual (0-indexed)\n        \"\"\"\n        self.current_epoch = epoch\n        \n        # Warm-up: incremento lineal del learning rate\n        if epoch < self.warmup_epochs:\n            # LR crece linealmente de 0 a initial_lr\n            self.current_lr = self.initial_lr * (epoch + 1) / self.warmup_epochs\n        else:\n            # Después del warm-up, mantener o decrementar\n            # Opción 1: Mantener constante\n            self.current_lr = self.initial_lr\n            \n            # Opción 2: Decaimiento exponencial (comentado por defecto)\n            # decay_rate = 0.5\n            # decay_epochs = 5\n            # epochs_after_warmup = epoch - self.warmup_epochs\n            # self.current_lr = self.initial_lr * (decay_rate ** (epochs_after_warmup / decay_epochs))\n        \n        # Aplicar nuevo learning rate\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = self.current_lr\n    \n    def get_learning_rate(self):\n        \"\"\"Retorna el learning rate actual.\"\"\"\n        return self.current_lr\n    \n    def _get_parameters(self):\n        \"\"\"Obtiene todos los parámetros del optimizer.\"\"\"\n        params = []\n        for param_group in self.optimizer.param_groups:\n            params.extend(param_group['params'])\n        return params\n    \n    def state_dict(self):\n        \"\"\"Guarda el estado del optimizer.\"\"\"\n        return {\n            'optimizer': self.optimizer.state_dict(),\n            'current_epoch': self.current_epoch,\n            'current_step': self.current_step,\n            'current_lr': self.current_lr\n        }\n    \n    def load_state_dict(self, state_dict):\n        \"\"\"Carga el estado del optimizer.\"\"\"\n        self.optimizer.load_state_dict(state_dict['optimizer'])\n        self.current_epoch = state_dict['current_epoch']\n        self.current_step = state_dict['current_step']\n        self.current_lr = state_dict['current_lr']\n\n\ndef build_optimizer(model, config):\n    \"\"\"\n    Construye el optimizer basado en la configuración.\n    \n    Args:\n        model: Modelo PyTorch\n        config: Config object\n        \n    Returns:\n        ScheduledOptimizer\n    \"\"\"\n    learner_type = (config['learner'] if config['learner'] else 'adam').lower()\n    learning_rate = config['learning_rate']\n\n    \n   \n    if learner_type =='adagrad':\n        base_optimizer = optim.Adagrad(\n            model.parameters(),\n            lr=learning_rate,\n            initial_accumulator_value=0.1\n        )\n    elif learner_type == 'adam':\n        base_optimizer = optim.Adam(\n            model.parameters(),\n            lr=learning_rate,\n            betas=(0.9, 0.999),\n            eps=1e-6\n        )\n    elif learner_type == 'sgd':\n        base_optimizer = optim.SGD(\n            model.parameters(),\n            lr=learning_rate,\n            momentum=0.9\n        )\n    else:\n        raise ValueError(f\"Optimizer desconocido: {learner_type}\")\n    \n    # Envolver en ScheduledOptimizer\n    scheduled_optimizer = ScheduledOptimizer(base_optimizer, config)\n    \n    return scheduled_optimizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T01:38:18.484372Z","iopub.execute_input":"2026-01-02T01:38:18.484627Z","iopub.status.idle":"2026-01-02T01:38:18.502960Z","shell.execute_reply.started":"2026-01-02T01:38:18.484606Z","shell.execute_reply":"2026-01-02T01:38:18.502222Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class Attention(nn.Module):\n    \"\"\"\n    Mecanismo de atención de Bahdanau para el modelo Pointer-Generator.\n    Calcula la distribución de atención sobre los encoder outputs.\n    \"\"\"\n    \n    def __init__(self, hidden_size, is_coverage=False):\n        \"\"\"\n        Args:\n            hidden_size: Dimensión del hidden state\n            is_coverage: Si se usa coverage mechanism\n        \"\"\"\n        super(Attention, self).__init__()\n        \n        self.hidden_size = hidden_size\n        self.is_coverage = is_coverage\n        \n        # Proyecciones para calcular attention scores\n        # encoder_outputs: (batch, src_len, hidden_size * 2) si bidireccional\n        self.W_h = nn.Linear(hidden_size * 2, hidden_size, bias=False)  # Para encoder outputs\n        self.W_s = nn.Linear(hidden_size, hidden_size, bias=True)       # Para decoder state\n        \n        # Coverage feature\n        if is_coverage:\n            self.W_c = nn.Linear(1, hidden_size, bias=False)\n        \n        # Proyección final\n        self.v = nn.Linear(hidden_size, 1, bias=False)\n    \n    def forward(self, decoder_state, encoder_outputs, encoder_mask, coverage=None):\n        \"\"\"\n        Args:\n            decoder_state: (batch_size, hidden_size) - Estado actual del decoder\n            encoder_outputs: (batch_size, src_len, hidden_size * 2) - Outputs del encoder\n            encoder_mask: (batch_size, src_len) - Máscara de padding (1 = válido, 0 = padding)\n            coverage: (batch_size, src_len) - Vector de coverage acumulado (opcional)\n            \n        Returns:\n            context_vector: (batch_size, hidden_size * 2) - Vector de contexto\n            attention_dist: (batch_size, src_len) - Distribución de atención\n            coverage: (batch_size, src_len) - Coverage actualizado (si is_coverage=True)\n        \"\"\"\n        batch_size, src_len, _ = encoder_outputs.size()\n        \n        # 1. Proyectar encoder outputs\n        encoder_features = self.W_h(encoder_outputs)  # (batch_size, src_len, hidden_size)\n        \n        # 2. Proyectar decoder state y expandir\n        decoder_features = self.W_s(decoder_state)  # (batch_size, hidden_size)\n        decoder_features = decoder_features.unsqueeze(1)  # (batch_size, 1, hidden_size)\n        decoder_features = decoder_features.expand(-1, src_len, -1)  # (batch_size, src_len, hidden_size)\n        \n        # 3. Calcular attention scores\n        attention_features = encoder_features + decoder_features  # (batch_size, src_len, hidden_size)\n        \n        # 4. Añadir coverage si está activado\n        if self.is_coverage and coverage is not None:\n            coverage_features = self.W_c(coverage.unsqueeze(2))  # (batch_size, src_len, hidden_size)\n            attention_features = attention_features + coverage_features\n        \n        # 5. Calcular scores\n        e = self.v(torch.tanh(attention_features))  # (batch_size, src_len, 1)\n        e = e.squeeze(2)  # (batch_size, src_len)\n        \n        # 6. Aplicar máscara (hacer -inf los padding para que softmax → 0)\n        e = e.masked_fill(encoder_mask == 0, -1e4)\n        \n        # 7. Softmax para obtener distribución de atención\n        attention_dist = F.softmax(e, dim=1)  # (batch_size, src_len)\n        \n        # 8. Calcular context vector\n        attention_dist_expanded = attention_dist.unsqueeze(1)  # (batch_size, 1, src_len)\n        context_vector = torch.bmm(attention_dist_expanded, encoder_outputs)  # (batch_size, 1, hidden_size * 2)\n        context_vector = context_vector.squeeze(1)  # (batch_size, hidden_size * 2)\n        \n        # 9. Actualizar coverage\n        if self.is_coverage:\n            if coverage is None:\n                coverage = attention_dist\n            else:\n                coverage = coverage + attention_dist\n        \n        return context_vector, attention_dist, coverage\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T01:38:18.503787Z","iopub.execute_input":"2026-01-02T01:38:18.504102Z","iopub.status.idle":"2026-01-02T01:38:18.521381Z","shell.execute_reply.started":"2026-01-02T01:38:18.504081Z","shell.execute_reply":"2026-01-02T01:38:18.520647Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass Encoder(nn.Module):\n    \"\"\"\n    Encoder bidireccional LSTM para el modelo Pointer-Generator.\n    \"\"\"\n    \n    def __init__(self, vocab_size, embedding_size, hidden_size, num_enc_layers, dropout_ratio, bidirectional, pretrained_weights=None):\n        \"\"\"\n        Args:\n            vocab_size: Tamaño del vocabulario base (sin OOVs)\n            embedding_size: Dimensión de los embeddings\n            hidden_size: Dimensión del hidden state del LSTM\n            num_enc_layers: Número de capas del LSTM\n            dropout_ratio: Probabilidad de dropout\n            bidirectional: Si el LSTM es bidireccional\n            pretrained_weights: Tensor con pesos pre-entrenados (opcional)\n        \"\"\"\n        super(Encoder, self).__init__()\n        \n        self.vocab_size = vocab_size\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.num_enc_layers = num_enc_layers\n        self.bidirectional = bidirectional\n        \n        # Embedding layer (solo para vocabulario base)\n        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        \n        if pretrained_weights is not None:\n            self.embedding.weight.data.copy_(pretrained_weights)\n            self.embedding.weight.requires_grad = False \n            print(\"✓ Encoder: Pesos de embedding inicializados (no Entrenables).\")\n        \n        # LSTM bidireccional\n        self.lstm = nn.LSTM(\n            input_size=embedding_size,\n            hidden_size=hidden_size,\n            num_layers=num_enc_layers,\n            batch_first=True,\n            bidirectional=bidirectional,\n            dropout=dropout_ratio if num_enc_layers > 1 else 0\n        )\n        self.dropout = nn.Dropout(dropout_ratio)\n        # Proyección para reducir hidden state bidireccional\n        if bidirectional:\n            self.reduce_h = nn.Linear(hidden_size * 2, hidden_size)\n            self.reduce_c = nn.Linear(hidden_size * 2, hidden_size)\n    \n    def forward(self, encoder_input, encoder_length):\n        \"\"\"\n        Args:\n            encoder_input: (batch_size, src_len) - IDs del vocabulario base (OOVs → UNK)\n            encoder_length: (batch_size,) - Longitudes reales de cada secuencia\n            \n        Returns:\n            encoder_outputs: (batch_size, src_len, hidden_size * 2) si bidirectional\n            hidden: Tuple (h_n, c_n) reducidos a (batch_size, hidden_size)\n        \"\"\"\n        batch_size, src_len = encoder_input.size()\n        \n        # 1. Embeddings\n        embedded = self.embedding(encoder_input)  # (batch_size, src_len, embedding_size)\n        embedded = self.dropout(embedded)\n        # 2. Pack padded sequence para eficiencia\n        packed = nn.utils.rnn.pack_padded_sequence(\n            embedded, \n            encoder_length.cpu(), \n            batch_first=True, \n            enforce_sorted=False\n        )\n        \n        # 3. LSTM\n        packed_outputs, (h_n, c_n) = self.lstm(packed)\n        \n        # 4. Unpack\n        encoder_outputs, _ = nn.utils.rnn.pad_packed_sequence(\n            packed_outputs, \n            batch_first=True,\n            total_length=src_len\n        )\n        # encoder_outputs: (batch_size, src_len, hidden_size * 2)\n        \n        # 5. Reducir hidden states si es bidireccional\n        if self.bidirectional:\n            # h_n: (num_layers * 2, batch_size, hidden_size)\n            # Tomar última capa: (2, batch_size, hidden_size)\n            h_n = h_n[-2:]  # [forward, backward] de la última capa\n            c_n = c_n[-2:]\n            \n            # Concatenar forward y backward\n            h_n = torch.cat([h_n[0], h_n[1]], dim=1)  # (batch_size, hidden_size * 2)\n            c_n = torch.cat([c_n[0], c_n[1]], dim=1)\n            \n            # Reducir a hidden_size\n            h_n = torch.relu(self.reduce_h(h_n))  # (batch_size, hidden_size)\n            c_n = torch.relu(self.reduce_c(c_n))  # (batch_size, hidden_size)\n            \n            # Añadir dimensión de capas\n            h_n = h_n.unsqueeze(0)  # (1, batch_size, hidden_size)\n            c_n = c_n.unsqueeze(0)\n        \n        return encoder_outputs, (h_n, c_n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T01:38:18.523075Z","iopub.execute_input":"2026-01-02T01:38:18.523304Z","iopub.status.idle":"2026-01-02T01:38:18.540143Z","shell.execute_reply.started":"2026-01-02T01:38:18.523286Z","shell.execute_reply":"2026-01-02T01:38:18.539465Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\n\nclass Decoder(nn.Module):\n    \"\"\"\n    Decoder LSTM con Pointer-Generator Network y Coverage Mechanism.\n    \"\"\"\n    \n    def __init__(self,\n                 vocab_size,\n                 embedding_size,\n                 hidden_size,\n                 num_dec_layers,\n                 dropout_ratio,\n                 is_attention=True,\n                 is_pgen=True,\n                 is_coverage=True):\n        \"\"\"\n        Args:\n            vocab_size: Tamaño del vocabulario base (sin OOVs)\n            embedding_size: Dimensión de los embeddings\n            hidden_size: Dimensión del hidden state del LSTM\n            num_dec_layers: Número de capas del LSTM (debe ser 1 para PGN)\n            dropout_ratio: Probabilidad de dropout\n            is_attention: Si se usa atención\n            is_pgen: Si se usa pointer-generator\n            is_coverage: Si se usa coverage mechanism\n        \"\"\"\n        super(Decoder, self).__init__()\n        \n        self.vocab_size = vocab_size\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.is_attention = is_attention\n        self.is_pgen = is_pgen\n        self.is_coverage = is_coverage\n        self.dropout = nn.Dropout(dropout_ratio)\n        # Embedding layer (solo para vocabulario base)\n        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        \n        # LSTM decoder\n        # Input: embedding + context vector (si hay atención)\n        lstm_input_size = embedding_size + (hidden_size * 2 if is_attention else 0)\n        \n        self.lstm = nn.LSTM(\n            input_size=lstm_input_size,\n            hidden_size=hidden_size,\n            num_layers=num_dec_layers,\n            batch_first=True,\n            dropout=dropout_ratio if num_dec_layers > 1 else 0\n        )\n        \n        # Attention mechanism\n        if is_attention:\n            self.attention = Attention(hidden_size, is_coverage=is_coverage)\n        \n        # Proyección para generar distribución de vocabulario\n        # Input: decoder state + context vector\n        vocab_input_size = hidden_size + (hidden_size * 2 if is_attention else 0)\n        self.vocab_proj = nn.Linear(vocab_input_size, vocab_size)\n        \n        # Pointer-Generator: calcular p_gen\n        if is_pgen:\n            # p_gen depende de: context vector, decoder state, decoder input\n            pgen_input_size = (hidden_size * 2) + hidden_size + embedding_size\n            self.p_gen_linear = nn.Linear(pgen_input_size, 1)\n    \n    def forward(self, decoder_input, decoder_state, encoder_outputs, encoder_mask, \n                extended_encoder_input, context_vector=None, coverage=None):\n        \"\"\"\n        Un paso de decodificación.\n        \n        Args:\n            decoder_input: (batch_size,) - Token actual (ID del vocabulario base)\n            decoder_state: Tuple (h, c) donde cada uno es (1, batch_size, hidden_size)\n            encoder_outputs: (batch_size, src_len, hidden_size * 2)\n            encoder_mask: (batch_size, src_len)\n            extended_encoder_input: (batch_size, src_len) - IDs extendidos del source\n            context_vector: (batch_size, hidden_size * 2) - Context vector del paso anterior\n            coverage: (batch_size, src_len) - Coverage acumulado\n            \n        Returns:\n            final_dist: (batch_size, extended_vocab_size) - Distribución final sobre vocab extendido\n            decoder_state: Tuple (h, c) actualizado\n            context_vector: (batch_size, hidden_size * 2) - Nuevo context vector\n            attention_dist: (batch_size, src_len) - Distribución de atención\n            p_gen: (batch_size, 1) - Probabilidad de generar (si is_pgen=True)\n            coverage: (batch_size, src_len) - Coverage actualizado\n        \"\"\"\n        batch_size = decoder_input.size(0)\n        \n        # 1. Embeddings del input\n        embedded = self.embedding(decoder_input)  # (batch_size, embedding_size)\n        embedded = self.dropout(embedded)\n        embedded = embedded.unsqueeze(1)  # (batch_size, 1, embedding_size)\n        \n        # 2. Concatenar con context vector si hay atención\n        if self.is_attention and context_vector is not None:\n            lstm_input = torch.cat([embedded, context_vector.unsqueeze(1)], dim=2)\n        else:\n            lstm_input = embedded\n        \n        # 3. LSTM step\n        lstm_output, decoder_state = self.lstm(lstm_input, decoder_state)\n        # lstm_output: (batch_size, 1, hidden_size)\n        lstm_output = lstm_output.squeeze(1)  # (batch_size, hidden_size)\n        \n        # 4. Calcular atención\n        if self.is_attention:\n            context_vector, attention_dist, coverage = self.attention(\n                lstm_output, encoder_outputs, encoder_mask, coverage\n            )\n            # context_vector: (batch_size, hidden_size * 2)\n            # attention_dist: (batch_size, src_len)\n        else:\n            attention_dist = None\n        \n        # 5. Generar distribución de vocabulario\n        if self.is_attention:\n            vocab_input = torch.cat([lstm_output, context_vector], dim=1)\n        else:\n            vocab_input = lstm_output\n        \n        vocab_logits = self.vocab_proj(vocab_input)  # (batch_size, vocab_size)\n        vocab_dist = F.softmax(vocab_logits, dim=1)  # (batch_size, vocab_size)\n        \n        # 6. Pointer-Generator mechanism\n        p_gen = None\n        if self.is_pgen and self.is_attention:\n            # Calcular p_gen\n            pgen_input = torch.cat([\n                context_vector,           # (batch_size, hidden_size * 2)\n                lstm_output,              # (batch_size, hidden_size)\n                embedded.squeeze(1)       # (batch_size, embedding_size)\n            ], dim=1)\n            \n            p_gen = torch.sigmoid(self.p_gen_linear(pgen_input))  # (batch_size, 1)\n            \n            # Combinar vocab_dist y attention_dist\n            # final_dist = p_gen * vocab_dist + (1 - p_gen) * copy_dist\n            \n            # Crear distribución extendida\n            src_len = extended_encoder_input.size(1)\n            extended_vocab_size = self.vocab_size + src_len  # Aproximación conservadora\n            \n            # Inicializar distribución extendida\n            final_dist = torch.zeros(batch_size, extended_vocab_size, device=vocab_dist.device)\n            \n            # Añadir vocab_dist ponderado por p_gen\n            final_dist[:, :self.vocab_size] = p_gen * vocab_dist\n            \n            # Añadir attention_dist ponderado por (1 - p_gen) usando scatter_add\n            # Copiar de las posiciones del source\n            attention_weighted = (1 - p_gen) * attention_dist  # (batch_size, src_len)\n            \n            # scatter_add para acumular probabilidades en posiciones extendidas\n            final_dist.scatter_add_(\n                dim=1,\n                index=extended_encoder_input,\n                src=attention_weighted\n            )\n        else:\n            final_dist = vocab_dist\n        \n        return final_dist, decoder_state, context_vector, attention_dist, p_gen, coverage","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2026-01-02T01:38:18.540877Z","iopub.execute_input":"2026-01-02T01:38:18.541162Z","iopub.status.idle":"2026-01-02T01:38:18.565944Z","shell.execute_reply.started":"2026-01-02T01:38:18.541143Z","shell.execute_reply":"2026-01-02T01:38:18.565188Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass PointerGeneratorNetwork(nn.Module):\n    \"\"\"\n    Pointer-Generator Network con Coverage Mechanism para text summarization.\n    \n    Referencia: \"Get To The Point: Summarization with Pointer-Generator Networks\"\n    (See et al., 2017) - https://arxiv.org/abs/1704.04368\n    \"\"\"\n    \n    def __init__(self, config, vocab, pretrained_weights=None):\n        \"\"\"\n        Args:\n            config: Objeto Config con hiperparámetros\n            vocab: Objeto Vocabulary\n            pretrained_weights: Tensor con pesos pre-entrenados (opcional)\n        \"\"\"\n        super(PointerGeneratorNetwork, self).__init__()\n        \n        self.config = config\n        self.vocab = vocab\n        self.vocab_size = config['max_vocab_size']\n        \n        # Encoder\n        self.encoder = Encoder(\n            vocab_size=self.vocab_size,\n            embedding_size=config['embedding_size'],\n            hidden_size=config['hidden_size'],\n            num_enc_layers=config['num_enc_layers'],  \n            dropout_ratio=config['dropout_ratio'],\n            bidirectional=config['bidirectional'],\n            pretrained_weights=pretrained_weights\n        )\n        \n        # Decoder\n        self.decoder = Decoder(\n            vocab_size=self.vocab_size,\n            embedding_size=config['embedding_size'],\n            hidden_size=config['hidden_size'],\n            num_dec_layers=config['num_dec_layers'],  \n            dropout_ratio=config['dropout_ratio'],\n            is_attention=True,\n            is_pgen=config['is_pgen'],\n            is_coverage=config['is_coverage']\n        )\n        \n        self.is_coverage = config['is_coverage']\n        self.coverage_lambda = config['coverage_lambda'] if config['coverage_lambda'] is not None else 1.0\n        self.device = config['device']\n        \n        # Compartir embeddings entre encoder y decoder\n        self.decoder.embedding.weight = self.encoder.embedding.weight\n    \n    def forward(self, batch, is_training=True):\n        \"\"\"\n        Forward pass completo para training.\n        \n        Args:\n            batch: Dict con:\n                - encoder_input: (batch_size, src_len) - IDs base\n                - extended_encoder_input: (batch_size, src_len) - IDs extendidos\n                - encoder_length: (batch_size,)\n                - encoder_mask: (batch_size, src_len)\n                - decoder_input: (batch_size, tgt_len) - IDs base\n                - decoder_target: (batch_size, tgt_len) - IDs extendidos\n            is_training: Si es modo training (teacher forcing)\n            \n        Returns:\n            Dict con:\n                - loss: Scalar tensor\n                - vocab_loss: Scalar tensor\n                - coverage_loss: Scalar tensor (si is_coverage=True)\n                - final_dists: (batch_size, tgt_len, extended_vocab_size)\n        \"\"\"\n        # Unpack batch\n        encoder_input = batch['encoder_input'].to(self.device)\n        extended_encoder_input = batch['extended_encoder_input'].to(self.device)\n        encoder_length = batch['encoder_length'].to(self.device)\n        encoder_mask = batch['encoder_mask'].to(self.device)\n        decoder_input = batch['decoder_input'].to(self.device)\n        decoder_target = batch['decoder_target'].to(self.device)\n        \n        batch_size, tgt_len = decoder_input.size()\n        src_len = encoder_input.size(1)\n        \n        # 1. Encoder\n        encoder_outputs, decoder_state = self.encoder(encoder_input, encoder_length)\n        # encoder_outputs: (batch_size, src_len, hidden_size * 2)\n        # decoder_state: Tuple (h, c) - (1, batch_size, hidden_size)\n        \n        # 2. Inicializar\n        context_vector = torch.zeros(batch_size, self.config['hidden_size'] * 2, device=self.device)\n        coverage = torch.zeros(batch_size, src_len, device=self.device) if self.is_coverage else None\n        \n        # 3. Decoder loop (teacher forcing)\n        final_dists = []\n        attention_dists = []\n        coverages = []\n        \n        for t in range(tgt_len):\n            decoder_input_t = decoder_input[:, t]  # (batch_size,)\n            \n            final_dist, decoder_state, context_vector, attention_dist, p_gen, coverage = self.decoder(\n                decoder_input=decoder_input_t,\n                decoder_state=decoder_state,\n                encoder_outputs=encoder_outputs,\n                encoder_mask=encoder_mask,\n                extended_encoder_input=extended_encoder_input,\n                context_vector=context_vector,\n                coverage=coverage\n            )\n            \n            final_dists.append(final_dist)\n            attention_dists.append(attention_dist)\n            if self.is_coverage:\n                coverages.append(coverage)\n        \n        # Stack outputs\n        final_dists = torch.stack(final_dists, dim=1)  # (batch_size, tgt_len, extended_vocab_size)\n        attention_dists = torch.stack(attention_dists, dim=1)  # (batch_size, tgt_len, src_len)\n        \n        # 4. Calcular loss\n        vocab_loss = self._calculate_vocab_loss(final_dists, decoder_target)\n        \n        coverage_loss = torch.tensor(0.0, device=self.device)\n        if self.is_coverage and len(coverages) > 0:\n            coverages = torch.stack(coverages, dim=1)  # (batch_size, tgt_len, src_len)\n            coverage_loss = self._calculate_coverage_loss(attention_dists, coverages, encoder_mask)\n        \n        # Loss total (aplicar lambda al coverage loss)\n        total_loss = vocab_loss + self.coverage_lambda * coverage_loss\n        \n        return {\n            'loss': total_loss,\n            'vocab_loss': vocab_loss,\n            'coverage_loss': coverage_loss,\n            'final_dists': final_dists\n        }\n    \n    def _calculate_vocab_loss(self, final_dists, targets):\n        \"\"\"\n        Calcula negative log likelihood loss.\n        \n        Args:\n            final_dists: (batch_size, tgt_len, extended_vocab_size)\n            targets: (batch_size, tgt_len) - IDs extendidos\n            \n        Returns:\n            loss: Scalar tensor\n        \"\"\"\n        batch_size, tgt_len, _ = final_dists.size()\n        \n        # Evitar log(0)\n        final_dists = final_dists + 1e-12\n        \n        # Gather las probabilidades de los targets\n        targets_expanded = targets.unsqueeze(2)  # (batch_size, tgt_len, 1)\n        \n        # Clamp targets para evitar índices fuera de rango\n        max_idx = final_dists.size(2) - 1\n        targets_clamped = torch.clamp(targets_expanded, 0, max_idx)\n        \n        probs = torch.gather(final_dists, dim=2, index=targets_clamped)  # (batch_size, tgt_len, 1)\n        probs = probs.squeeze(2)  # (batch_size, tgt_len)\n        \n        # Negative log likelihood\n        losses = -torch.log(probs)\n        \n        # Máscara de padding (PAD_ID = 0)\n        mask = (targets != 0).float()\n        \n        # Loss promedio sobre tokens no-padding\n        loss = (losses * mask).sum() / mask.sum()\n        \n        return loss\n    \n    def _calculate_coverage_loss(self, attention_dists, coverages, encoder_mask):\n        \"\"\"\n        Calcula coverage loss para penalizar atención repetida.\n        \n        Args:\n            attention_dists: (batch_size, tgt_len, src_len)\n            coverages: (batch_size, tgt_len, src_len) - Coverage en cada paso\n            encoder_mask: (batch_size, src_len)\n            \n        Returns:\n            coverage_loss: Scalar tensor\n        \"\"\"\n        # Coverage loss = sum_t min(a_t, c_t)\n        # Penaliza cuando atendemos posiciones ya atendidas\n        \n        # Shift coverage: usamos coverage del paso anterior\n        coverage_prev = torch.cat([\n            torch.zeros_like(coverages[:, :1, :]),  # t=0 no tiene coverage previo\n            coverages[:, :-1, :]  # t>0 usa coverage de t-1\n        ], dim=1)\n        \n        # min(attention, coverage_prev)\n        min_vals = torch.min(attention_dists, coverage_prev)\n        \n        # Sumar sobre src_len y tgt_len, aplicar máscara\n        encoder_mask_expanded = encoder_mask.unsqueeze(1)  # (batch_size, 1, src_len)\n        \n        coverage_loss = (min_vals * encoder_mask_expanded.float()).sum()\n        \n        # Normalizar por número de tokens\n        num_tokens = encoder_mask.sum()\n        coverage_loss = coverage_loss / num_tokens\n        \n        return coverage_loss\n    \n    def decode_greedy(self, batch, max_len=None):\n        \"\"\"\n        Decodificación greedy (sin beam search).\n        \n        Args:\n            batch: Dict con encoder inputs\n            max_len: Longitud máxima de generación\n            \n        Returns:\n            generated_ids: (batch_size, max_len) - Secuencia generada\n        \"\"\"\n        if max_len is None:\n            max_len = self.config['tgt_len']\n        \n        # Unpack\n        encoder_input = batch['encoder_input'].to(self.device)\n        extended_encoder_input = batch['extended_encoder_input'].to(self.device)\n        encoder_length = batch['encoder_length'].to(self.device)\n        encoder_mask = batch['encoder_mask'].to(self.device)\n        \n        batch_size = encoder_input.size(0)\n        src_len = encoder_input.size(1)\n        \n        # Encoder\n        encoder_outputs, decoder_state = self.encoder(encoder_input, encoder_length)\n        \n        # Inicializar\n        context_vector = torch.zeros(batch_size, self.config['hidden_size'] * 2, device=self.device)\n        coverage = torch.zeros(batch_size, src_len, device=self.device) if self.is_coverage else None\n        \n        # Start token\n        decoder_input = torch.full(\n            (batch_size,), \n            self.vocab.word2id(self.vocab.start_decoding),\n            dtype=torch.long,\n            device=self.device\n        )\n        \n        generated_ids = []\n        p_gens = []\n                \n        for t in range(max_len):\n            final_dist, decoder_state, context_vector, attention_dist, p_gen, coverage = self.decoder(\n                decoder_input=decoder_input,\n                decoder_state=decoder_state,\n                encoder_outputs=encoder_outputs,\n                encoder_mask=encoder_mask,\n                extended_encoder_input=extended_encoder_input,\n                context_vector=context_vector,\n                coverage=coverage\n            )\n            \n            # Greedy: seleccionar el token con mayor probabilidad\n            predicted_ids = torch.argmax(final_dist, dim=1)  # (batch_size,)\n            generated_ids.append(predicted_ids)\n            \n            # Guardamos p_gen (si es None por no-pgen, guardamos 1.0 = generación pura)\n            if p_gen is not None:\n                p_gens.append(p_gen)\n            else:\n                p_gens.append(torch.ones(batch_size, 1, device=self.device))\n            \n            # Próximo input: convertir OOVs a UNK\n            decoder_input = torch.where(\n                predicted_ids < self.vocab_size,\n                predicted_ids,\n                torch.full_like(predicted_ids, self.vocab.word2id(self.vocab.unk_token))\n            )\n        \n        generated_ids = torch.stack(generated_ids, dim=1)  # (batch_size, max_len)\n        p_gens = torch.stack(p_gens, dim=1) # (batch_size, max_len, 1)\n        return generated_ids, p_gens\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T01:38:18.566736Z","iopub.execute_input":"2026-01-02T01:38:18.566952Z","iopub.status.idle":"2026-01-02T01:38:18.596720Z","shell.execute_reply.started":"2026-01-02T01:38:18.566933Z","shell.execute_reply":"2026-01-02T01:38:18.595967Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport os\nimport time\nfrom tqdm import tqdm\nimport json\n\nclass Trainer:\n    \"\"\"\n    Clase para entrenar el modelo Pointer-Generator Network.\n    \"\"\"\n    \n    def __init__(self, config, vocab, pretrained_weights=None):\n        \"\"\"\n        Args:\n            config: Config object\n            vocab: Vocabulary object\n            pretrained_weights: Tensor con pesos pre-entrenados (opcional)\n        \"\"\"\n        self.config = config\n        self.vocab = vocab\n        self.device = config['device']\n        \n        # Modelo\n        self.model = PointerGeneratorNetwork(config, vocab, pretrained_weights).to(self.device)\n        \n        # Optimizer\n        self.optimizer = build_optimizer(self.model, config)\n        \n        # Beam search para validación\n        self.beam_search = BeamSearch(\n            self.model,\n            vocab,\n            beam_size=config['beam_size'],\n            max_len=config['tgt_len']\n        )\n        \n        # Tracking\n        self.start_epoch = 0\n        self.global_step = 0\n        self.best_val_loss = float('inf')\n        \n        # History\n        self.train_history = {\n            'epoch': [],\n            'train_loss': [],\n            'vocab_loss': [],\n            'coverage_loss': [],\n            'val_loss': []\n        }\n        \n        # Paths\n        self.checkpoint_dir = config['checkpoint_dir']\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n        \n        # AMP Scaler\n        self.scaler = torch.amp.GradScaler('cuda', enabled=config['use_gpu'])\n    \n    def train(self, train_loader, val_loader, num_epochs):\n        \"\"\"\n        Entrena el modelo.\n        \n        Args:\n            train_loader: DataLoader de entrenamiento\n            val_loader: DataLoader de validación\n            num_epochs: Número de épocas\n        \"\"\"\n        print(f\"\\n{'='*60}\")\n        print(f\"Iniciando entrenamiento\")\n        print(f\"{'='*60}\")\n        print(f\"Device: {self.device}\")\n        print(f\"Épocas: {num_epochs}\")\n        print(f\"Batch size: {self.config['train_batch_size']}\")\n        print(f\"Learning rate: {self.config['learning_rate']}\")\n        print(f\"Pointer-Generator: {self.config['is_pgen']}\")\n        print(f\"Coverage: {self.config['is_coverage']}\")\n        print(f\"{'='*60}\\n\")\n        \n        for epoch in range(self.start_epoch, num_epochs):\n            epoch_start_time = time.time()\n            \n            # Actualizar learning rate\n            self.optimizer.update_learning_rate(epoch)\n            current_lr = self.optimizer.get_learning_rate()\n            \n            print(f\"\\n--- Epoch {epoch+1}/{num_epochs} (LR: {current_lr:.6f}) ---\")\n            \n            # Training\n            train_metrics = self._train_epoch(train_loader, epoch)\n            \n            # Validation\n            val_metrics = self._validate_epoch(val_loader, epoch)\n            \n            # Guardar history\n            self.train_history['epoch'].append(epoch + 1)\n            self.train_history['train_loss'].append(train_metrics['loss'])\n            self.train_history['vocab_loss'].append(train_metrics['vocab_loss'])\n            self.train_history['coverage_loss'].append(train_metrics['coverage_loss'])\n            self.train_history['val_loss'].append(val_metrics['loss'])\n            \n            # Guardar checkpoint\n            is_best = val_metrics['loss'] < self.best_val_loss\n            if is_best:\n                self.best_val_loss = val_metrics['loss']\n            \n            save_model = self.config['save_model_epoch'] if self.config['save_model_epoch'] is not None else True\n            if save_model:\n                self._save_checkpoint(epoch, is_best)\n            \n            # Tiempo\n            epoch_time = time.time() - epoch_start_time\n            print(f\"Tiempo de época: {epoch_time/60:.2f} min\")\n            \n        # Guardar history final\n        save_hist = self.config['save_history'] if self.config['save_history'] is not None else True\n        if save_hist:\n            self._save_history()\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Entrenamiento completado!\")\n        print(f\"Mejor Val Loss: {self.best_val_loss:.4f}\")\n        print(f\"{'='*60}\\n\")\n    \n    def _train_epoch(self, train_loader, epoch):\n        \"\"\"Entrena una época.\"\"\"\n        self.model.train()\n        \n        total_loss = 0.0\n        total_vocab_loss = 0.0\n        total_coverage_loss = 0.0\n        num_batches = 0\n        \n        # Progress bar\n        pbar = tqdm(train_loader, desc=f\"Training\", total=len(train_loader))\n        \n        for batch_idx, batch in enumerate(pbar):\n            if batch is None:\n                continue\n            self.optimizer.zero_grad()\n            \n            # Forward pass (con AMP)\n            with torch.amp.autocast('cuda', enabled=self.config['use_gpu']):\n                outputs = self.model(batch, is_training=True)\n                loss = outputs['loss']\n            \n            # Backward pass (con Scaler)\n            self.scaler.scale(loss).backward()\n            \n            # Optimizer step (con gradient clipping y Scaler)\n            # Desescalar gradientes par clipping\n            self.scaler.unscale_(self.optimizer.optimizer)\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['grad_clip'])\n            \n            self.scaler.step(self.optimizer.optimizer)\n            self.scaler.update()\n            \n            # Actualizar contador de pasos del wrapper (ya que no llamamos a self.optimizer.step())\n            self.optimizer.current_step += 1\n            \n            # Tracking\n            total_loss += loss.item()\n            total_vocab_loss += outputs['vocab_loss'].item()\n            total_coverage_loss += outputs['coverage_loss'].item()\n            num_batches += 1\n            self.global_step += 1\n            \n            # Update progress bar\n            pbar.set_postfix({\n                'loss': f\"{loss.item():.4f}\",\n                'vocab': f\"{outputs['vocab_loss'].item():.4f}\",\n                'cov': f\"{outputs['coverage_loss'].item():.4f}\"\n            })\n            \n            # Limitar iteraciones por época si está configurado\n            iters_per_epoch = self.config['iters_per_epoch']\n            if iters_per_epoch and batch_idx >= iters_per_epoch:\n                break\n        \n        pbar.close()\n        \n        # Promedios\n        avg_loss = total_loss / num_batches\n        avg_vocab_loss = total_vocab_loss / num_batches\n        avg_coverage_loss = total_coverage_loss / num_batches\n        \n        print(f\"Train Loss: {avg_loss:.4f} | \"\n              f\"Vocab: {avg_vocab_loss:.4f} | \"\n              f\"Coverage: {avg_coverage_loss:.4f}\")\n        \n        return {\n            'loss': avg_loss,\n            'vocab_loss': avg_vocab_loss,\n            'coverage_loss': avg_coverage_loss\n        }\n    \n    def _validate_epoch(self, val_loader, epoch):\n        \"\"\"Valida el modelo.\"\"\"\n        self.model.eval()\n        \n        total_loss = 0.0\n        total_vocab_loss = 0.0\n        total_coverage_loss = 0.0\n        num_batches = 0\n        \n        with torch.no_grad():\n            pbar = tqdm(val_loader, desc=f\"Validation\", total=len(val_loader))\n            \n            for batch in pbar:\n                if batch is None:\n                    continue\n                outputs = self.model(batch, is_training=True)\n                \n                total_loss += outputs['loss'].item()\n                total_vocab_loss += outputs['vocab_loss'].item()\n                total_coverage_loss += outputs['coverage_loss'].item()\n                num_batches += 1\n                \n                pbar.set_postfix({\n                    'val_loss': f\"{outputs['loss'].item():.4f}\"\n                })\n            \n            pbar.close()\n        \n        # Promedios\n        avg_loss = total_loss / num_batches\n        avg_vocab_loss = total_vocab_loss / num_batches\n        avg_coverage_loss = total_coverage_loss / num_batches\n        \n        print(f\"Val Loss: {avg_loss:.4f} | \"\n              f\"Vocab: {avg_vocab_loss:.4f} | \"\n              f\"Coverage: {avg_coverage_loss:.4f}\")\n        \n        return {\n            'loss': avg_loss,\n            'vocab_loss': avg_vocab_loss,\n            'coverage_loss': avg_coverage_loss\n        }\n    \n    def _save_checkpoint(self, epoch, is_best=False):\n        \"\"\"Guarda un checkpoint del modelo.\"\"\"\n        checkpoint = {\n            'epoch': epoch + 1,\n            'global_step': self.global_step,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'best_val_loss': self.best_val_loss,\n            'config': self.config.get_config_dict(),\n            'train_history': self.train_history\n        }\n        \n        \n        # Guardar último checkpoint (sobreescribe)\n        checkpoint_path = os.path.join(self.checkpoint_dir, 'checkpoint_last2.pt')\n        torch.save(checkpoint, checkpoint_path)\n        \n        # Guardar mejor checkpoint\n        if is_best:\n            best_path = os.path.join(self.checkpoint_dir, 'checkpoint_best2.pt')\n            torch.save(checkpoint, best_path)\n            print(f\"✓ Guardado mejor modelo (Val Loss: {self.best_val_loss:.4f})\")\n    \n    def _save_history(self):\n        \"\"\"Guarda el historial de entrenamiento.\"\"\"\n        history_path = os.path.join(self.checkpoint_dir, 'train_history.json')\n        with open(history_path, 'w') as f:\n            json.dump(self.train_history, f, indent=2)\n        print(f\"✓ Historial guardado en {history_path}\")\n    \n    def load_checkpoint(self, checkpoint_path):\n        \"\"\"\n        Carga un checkpoint.\n        \n        Args:\n            checkpoint_path: Ruta al checkpoint\n        \"\"\"\n        if not os.path.exists(checkpoint_path):\n            print(f\"⚠ Checkpoint no encontrado: {checkpoint_path}\")\n            return\n        \n        print(f\"Cargando checkpoint desde {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n        \n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.start_epoch = checkpoint['epoch']\n        self.global_step = checkpoint['global_step']\n        self.best_val_loss = checkpoint['best_val_loss']\n        self.train_history = checkpoint.get('train_history', self.train_history)\n        \n        print(f\"✓ Checkpoint cargado (Epoch {self.start_epoch}, Step {self.global_step})\")\n    \n    def find_latest_checkpoint(self):\n        \"\"\"Busca el checkpoint más reciente.\"\"\"\n        if not os.path.exists(self.checkpoint_dir):\n            return None\n        \n        # Buscar checkpoint_last.pt primero\n        last_checkpoint = os.path.join(self.checkpoint_dir, 'checkpoint_last2.pt')\n        if os.path.exists(last_checkpoint):\n            return last_checkpoint\n        \n        # Si no existe, buscar el checkpoint de época más reciente\n        epoch_checkpoints = []\n        for filename in os.listdir(self.checkpoint_dir):\n            if filename.startswith('checkpoint_epoch_x2') and filename.endswith('.pt'):\n                try:\n                    epoch_num = int(filename.replace('checkpoint_epoch_x2', '').replace('.pt', ''))\n                    epoch_checkpoints.append((epoch_num, os.path.join(self.checkpoint_dir, filename)))\n                except ValueError:\n                    continue\n        \n        if epoch_checkpoints:\n            # Retornar el de mayor número de época\n            epoch_checkpoints.sort(reverse=True)\n            return epoch_checkpoints[0][1]\n        \n        return None\n\n\ndef main():\n    \"\"\"\n    Función principal para entrenar el modelo.\n    \"\"\"\n    # 1. Configurar reproducibilidad\n    if REPRODUCIBILITY:\n        torch.manual_seed(SEED)\n        torch.cuda.manual_seed_all(SEED)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    \n    # 2. Crear vocabulario\n    print(\"Construyendo vocabulario...\")\n    vocab = Vocabulary(\n        CREATE_VOCABULARY=CREATE_VOCABULARY,\n        PAD_TOKEN=PAD_TOKEN,\n        UNK_TOKEN=UNK_TOKEN,\n        START_DECODING=START_DECODING,\n        END_DECODING=END_DECODING,\n        MAX_VOCAB_SIZE=MAX_VOCAB_SIZE,\n        CHECKPOINT_VOCABULARY_DIR=CHECKPOINT_VOCABULARY_DIR,\n        DATA_DIR=DATA_DIR,\n        VOCAB_NAME=VOCAB_NAME\n    )\n    vocab.build_vocabulary()\n    print(f\"✓ Vocabulario construido: {vocab.total_size()} palabras\")\n    \n    # 3. Configurar modelo\n    config = Config(\n        max_vocab_size=vocab.total_size(),\n        src_len=MAX_LEN_SRC,\n        tgt_len=MAX_LEN_TGT,\n        embedding_size=EMBEDDING_SIZE,\n        hidden_size=HIDDEN_SIZE,\n        num_enc_layers=NUM_ENC_LAYERS,\n        num_dec_layers=NUM_DEC_LAYERS,\n        use_gpu=USE_GPU,\n        is_pgen=IS_PGEN,\n        is_coverage=IS_COVERAGE,\n        coverage_lambda=COV_LOSS_LAMBDA,\n        grad_clip=GRAD_CLIP,\n        epochs=EPOCHS,\n        data_path=DATA_DIR,\n        generated_text_dir=GENERATED_TEXT_DIR,\n        checkpoint_dir=CHECKPOINT_DIR,\n        reproducibility=REPRODUCIBILITY,\n        plot=PLOT,\n        dropout_ratio=DROPOUT_RATIO,\n        bidirectional=BIDIRECTIONAL,\n        save_history=SAVE_HISTORY,\n        save_model_epoch=SAVE_MODEL_EPOCH,\n        seed=SEED,\n        device=DEVICE,\n        decoding_strategy=DECODING_STRATEGY,\n        beam_size=BEAM_SIZE,\n        train_batch_size=TRAIN_BATCH_SIZE,\n        eval_batch_size=EVAL_BATCH_SIZE,\n        learner=LEARNER,\n        learning_rate=LEARNING_RATE,\n        iters_per_epoch=ITERS_PER_EPOCH,\n        gpu_id=GPU_ID,\n        warmup_epochs=WARMUP_EPOCHS,\n        embedding_path=EMBEDDING_PATH\n    )\n    \n    print(config)\n\n    # 4. Cargar embeddings pre-entrenados (OPCIONAL)\n    pretrained_weights = None\n    if config['embedding_path'] is not None:\n        pretrained_weights = vocab.load_pretrained_embeddings(\n            config['embedding_path'], \n            config['embedding_size']\n        )\n    \n    # 5. Crear datasets\n    print(\"\\nCargando datasets...\")\n    train_dataset = PGNDataset(\n        vocab=vocab,\n        MAX_LEN_SRC=config['src_len'],\n        MAX_LEN_TGT=config['tgt_len'],\n        data_dir=config['data_path'],\n        split='train'\n    )\n    \n    val_dataset = PGNDataset(\n        vocab=vocab,\n        MAX_LEN_SRC=config['src_len'],\n        MAX_LEN_TGT=config['tgt_len'],\n        data_dir=config['data_path'],\n        split='val'\n    )\n    \n    print(f\"✓ Train: {len(train_dataset)} ejemplos\")\n    print(f\"✓ Val: {len(val_dataset)} ejemplos\")\n    \n    # 5. Crear DataLoaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['train_batch_size'],\n        shuffle=True,\n        collate_fn=pgn_collate_fn,\n        num_workers=2,  # Optimizado para Kaggle (2-4 suele ser ideal)\n        pin_memory=True if USE_GPU else False\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config['eval_batch_size'],\n        shuffle=False,\n        collate_fn=pgn_collate_fn,\n        num_workers=2,\n        pin_memory=True if USE_GPU else False\n    )\n    \n    # 6. Crear trainer\n    trainer = Trainer(config, vocab, pretrained_weights)\n    \n    # 7. Buscar y cargar checkpoint automáticamente\n    latest_checkpoint = trainer.find_latest_checkpoint()\n    if latest_checkpoint:\n        print(f\"\\n🔄 Checkpoint encontrado: {latest_checkpoint}\")\n        trainer.load_checkpoint(latest_checkpoint)\n        print(f\"Continuando desde época {trainer.start_epoch}\\n\")\n    else:\n        print(\"\\n🆕 No se encontró checkpoint previo. Iniciando entrenamiento desde cero.\\n\")\n    \n    # 8. Entrenar\n    trainer.train(\n        train_loader=train_loader,\n        val_loader=val_loader,\n        num_epochs=config['epochs']\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T01:38:18.638183Z","iopub.execute_input":"2026-01-02T01:38:18.638425Z","iopub.status.idle":"2026-01-02T07:37:13.658127Z","shell.execute_reply.started":"2026-01-02T01:38:18.638406Z","shell.execute_reply":"2026-01-02T07:37:13.657439Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Construyendo vocabulario...\nDescargando modelo de spaCy español (Large)...\nCollecting es-core-news-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/12.9 MB 31.8 MB/s eta 0:00:00\nInstalling collected packages: es-core-news-sm\nSuccessfully installed es-core-news-sm-3.8.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('es_core_news_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\nsaved/working/Vocabulary.json\n Vocabulario cargado desde: saved/working/Vocabulary.json\n Tamaño total: 50000\n Tokens especiales: 4\n Tokens regulares: 49996\n✓ Vocabulario construido: 50000 palabras\n\nHyper Parameters:\nmax_vocab_size=50000\nsrc_len=500\ntgt_len=50\nembedding_size=300\nhidden_size=256\nnum_enc_layers=1\nnum_dec_layers=1\nuse_gpu=True\nis_pgen=True\nis_coverage=True\ncoverage_lambda=1.0\ngrad_clip=2.0\nepochs=15\ndata_path=/kaggle/input/msum-sum/es\ngenerated_text_dir=kaggle/working/generated\ncheckpoint_dir=kaggle/working/saved\nreproducibility=False\nplot=False\ndropout_ratio=0.3\nbidirectional=True\nsave_history=True\nsave_model_epoch=True\nseed=42\ndevice=cuda:0\ndecoding_strategy=beam_search\nbeam_size=5\ntrain_batch_size=64\neval_batch_size=64\nlearner=adam\nlearning_rate=0.001\niters_per_epoch=None\ngpu_id=0\nwarmup_epochs=0\nembedding_path=kaggle/working/wiki.es.vec\n\nCargando embeddings (Solo Coincidencias Exactas) desde kaggle/working/wiki.es.vec...\n","output_type":"stream"},{"name":"stderr","text":"Alineando embeddings: 2000000it [00:31, 64001.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"✓ Cobertura Exacta: 49958 / 50000 (99.9%)\n  - Las 42 palabras restantes serán aprendidas desde cero.\n\nCargando datasets...\n✓ Usando archivos TOKENIZADOS para train (Carga optimizada en Kaggle)\n✓ Usando archivos TOKENIZADOS para val (Carga optimizada en Kaggle)\n✓ Train: 266367 ejemplos\n✓ Val: 10358 ejemplos\n✓ Encoder: Pesos de embedding inicializados (no Entrenables).\n\n🔄 Checkpoint encontrado: kaggle/working/saved/checkpoint_last2.pt\nCargando checkpoint desde kaggle/working/saved/checkpoint_last2.pt\n✓ Checkpoint cargado (Epoch 6, Step 24972)\nContinuando desde época 6\n\n\n============================================================\nIniciando entrenamiento\n============================================================\nDevice: cuda:0\nÉpocas: 15\nBatch size: 64\nLearning rate: 0.001\nPointer-Generator: True\nCoverage: True\n============================================================\n\n\n--- Epoch 7/15 (LR: 0.001000) ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 4162/4162 [39:14<00:00,  1.77it/s, loss=3.0947, vocab=3.0790, cov=0.0158]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.9726 | Vocab: 2.9540 | Coverage: 0.0186\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 162/162 [00:32<00:00,  4.98it/s, val_loss=3.4514]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.2078 | Vocab: 3.1891 | Coverage: 0.0187\nTiempo de época: 39.81 min\n\n--- Epoch 8/15 (LR: 0.001000) ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 4162/4162 [39:14<00:00,  1.77it/s, loss=2.6625, vocab=2.6477, cov=0.0149]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.9168 | Vocab: 2.8985 | Coverage: 0.0183\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 162/162 [00:32<00:00,  5.00it/s, val_loss=3.4433]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.2070 | Vocab: 3.1886 | Coverage: 0.0183\nTiempo de época: 39.81 min\n\n--- Epoch 9/15 (LR: 0.001000) ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 4162/4162 [39:10<00:00,  1.77it/s, loss=2.7482, vocab=2.7289, cov=0.0193]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.8641 | Vocab: 2.8462 | Coverage: 0.0179\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 162/162 [00:32<00:00,  4.98it/s, val_loss=3.4268]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.2248 | Vocab: 3.2046 | Coverage: 0.0201\nTiempo de época: 39.74 min\n\n--- Epoch 10/15 (LR: 0.001000) ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 4162/4162 [39:15<00:00,  1.77it/s, loss=2.6322, vocab=2.6147, cov=0.0175]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.8234 | Vocab: 2.8055 | Coverage: 0.0179\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 162/162 [00:32<00:00,  5.01it/s, val_loss=3.4530]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.2376 | Vocab: 3.2173 | Coverage: 0.0203\nTiempo de época: 39.83 min\n\n--- Epoch 11/15 (LR: 0.001000) ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 4162/4162 [39:14<00:00,  1.77it/s, loss=2.8434, vocab=2.8203, cov=0.0231]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.8223 | Vocab: 2.8010 | Coverage: 0.0212\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 162/162 [00:32<00:00,  5.03it/s, val_loss=3.4762]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.2557 | Vocab: 3.2359 | Coverage: 0.0197\nTiempo de época: 39.80 min\n\n--- Epoch 12/15 (LR: 0.001000) ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 4162/4162 [39:11<00:00,  1.77it/s, loss=2.8557, vocab=2.8360, cov=0.0197]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.8017 | Vocab: 2.7812 | Coverage: 0.0205\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 162/162 [00:32<00:00,  5.02it/s, val_loss=3.4779]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.2506 | Vocab: 3.2301 | Coverage: 0.0204\nTiempo de época: 39.75 min\n\n--- Epoch 13/15 (LR: 0.001000) ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 4162/4162 [39:12<00:00,  1.77it/s, loss=2.8316, vocab=2.8164, cov=0.0152]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.7505 | Vocab: 2.7305 | Coverage: 0.0200\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 162/162 [00:32<00:00,  4.98it/s, val_loss=3.4756]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.2620 | Vocab: 3.2413 | Coverage: 0.0207\nTiempo de época: 39.77 min\n\n--- Epoch 14/15 (LR: 0.001000) ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 4162/4162 [39:13<00:00,  1.77it/s, loss=2.6551, vocab=2.6352, cov=0.0199]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.7245 | Vocab: 2.7052 | Coverage: 0.0193\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 162/162 [00:32<00:00,  4.97it/s, val_loss=3.5223]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.2808 | Vocab: 3.2597 | Coverage: 0.0211\nTiempo de época: 39.78 min\n\n--- Epoch 15/15 (LR: 0.001000) ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 4162/4162 [39:17<00:00,  1.77it/s, loss=2.9747, vocab=2.9342, cov=0.0404]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.7592 | Vocab: 2.7322 | Coverage: 0.0270\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 162/162 [00:32<00:00,  5.00it/s, val_loss=3.5651]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.3569 | Vocab: 3.3192 | Coverage: 0.0377\nTiempo de época: 39.86 min\n✓ Historial guardado en kaggle/working/saved/train_history.json\n\n============================================================\nEntrenamiento completado!\nMejor Val Loss: 3.2034\n============================================================\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# generar","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport sys\n\ndef analyze_missing_words():\n    vocab_path = os.path.join(CHECKPOINT_VOCABULARY_DIR, VOCAB_NAME)\n    \n    if not os.path.exists(vocab_path):\n        print(f\"✗ No se encontró el vocabulario en {vocab_path}. Asegúrate de haberlo construido antes.\")\n        return\n\n    print(f\"Cargando vocabulario desde {vocab_path}...\")\n    with open(vocab_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    word_to_id = data['word_to_id']\n    id_to_word = data['id_to_word']\n    word_count = data['word_count']\n    \n    if not os.path.exists(EMBEDDING_PATH):\n        print(f\"✗ No se encontró el archivo de embeddings en {EMBEDDING_PATH}\")\n        return\n\n    print(f\"Leyendo archivo de embeddings para identificar palabras presentes...\")\n    words_in_embeddings = set()\n    a=[]\n    s=1\n    try:\n        with open(EMBEDDING_PATH, 'r', encoding='utf-8', errors='ignore') as f:\n            # Saltar header si existe\n            header = f.readline().split()\n            if len(header) != 2:\n                f.seek(0)\n            \n            for line in f:\n                parts = line.rstrip().split(' ')\n                words_in_embeddings.add(parts[0])\n                if s==1:\n                    s+=1\n                    a.append((parts[0],parts[1:301]))\n    except Exception as e:\n        print(f\"✗ Error leyendo embeddings: {e}\")\n        return\n\n    missing_words = []\n    found_count = 0\n    \n    for word in id_to_word:\n        # Ignorar tokens especiales\n        if word.startswith('[') and word.endswith(']'):\n            continue\n            \n        if word in words_in_embeddings:\n            found_count += 1\n        else:\n            count = word_count.get(word, 0)\n            missing_words.append((word, count))\n\n    # Ordenar palabras faltantes por frecuencia\n    missing_words.sort(key=lambda x: x[1], reverse=True)\n\n    total_vocab = len(id_to_word) - 4 # Descontar tokens especiales\n    print(f\"\\n{'='*60}\")\n    print(f\"ANÁLISIS DE COBERTURA\")\n    print(f\"{'='*60}\")\n    print(f\"Total palabras (sin especiales): {total_vocab}\")\n    print(f\"Palabras encontradas:            {found_count} ({found_count/total_vocab*100:.2f}%)\")\n    print(f\"Palabras FALTANTES:              {len(missing_words)} ({len(missing_words)/total_vocab*100:.2f}%)\")\n    print(f\"{'='*60}\")\n    \n    print(\"\\nTOP 200 PALABRAS MÁS FRECUENTES FALTANTES EN EMBEDDINGS:\")\n    print(f\"{'Palabra':<30} | {'Frecuencia':<10}\")\n    print(\"-\" * 45)\n    for word, count in missing_words[:200]:\n        print(f\"{word:<30} | {count:<10}\")\n    \n\n    # Análisis de causas comunes\n    casing_issues = sum(1 for w, c in missing_words if w.lower() in words_in_embeddings and w != w.lower())\n    print(f\"\\nPosibles mejoras:\")\n    print(f\"- {casing_issues} palabras podrían encontrarse si pasamos todo a minúsculas.\")\n    print(a[0])\nif __name__ == \"__main__\":\n    analyze_missing_words()\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2026-01-02T07:37:13.818198Z","iopub.execute_input":"2026-01-02T07:37:13.819085Z","iopub.status.idle":"2026-01-02T07:37:42.941637Z","shell.execute_reply.started":"2026-01-02T07:37:13.819031Z","shell.execute_reply":"2026-01-02T07:37:42.940866Z"}},"outputs":[{"name":"stdout","text":"Cargando vocabulario desde saved/working/Vocabulary.json...\nLeyendo archivo de embeddings para identificar palabras presentes...\n\n============================================================\nANÁLISIS DE COBERTURA\n============================================================\nTotal palabras (sin especiales): 49996\nPalabras encontradas:            49958 (99.92%)\nPalabras FALTANTES:              38 (0.08%)\n============================================================\n\nTOP 200 PALABRAS MÁS FRECUENTES FALTANTES EN EMBEDDINGS:\nPalabra                        | Frecuencia\n---------------------------------------------\nI+D                            | 1849      \nd'Esquadra                     | 1591      \n-M.                            | 907       \nI+D+i                          | 820       \nelpaissemanal                  | 756       \nREUTERS-QUALITY                | 700       \nn't                            | 678       \nkm/h                           | 660       \n-year-old                      | 639       \n-S.                            | 596       \n-N.                            | 560       \nL'Hospitalet                   | 553       \nCanal+                         | 534       \nCamhaji                        | 491       \n-D.                            | 455       \nMeToo                          | 373       \npro-independence               | 314       \nReuters-Quality                | 290       \nd'Hebron                       | 272       \nNadalº                         | 270       \njitorreblanca                  | 234       \nREUTERS-LIVE                   | 233       \nMovistar+                      | 233       \nSatty                          | 233       \ninvestiture                    | 226       \nC/                             | 223       \nseparatist                     | 221       \nParal·lel                      | 195       \nmás                           | 192       \nMinocri                        | 182       \nºsaca                          | 174       \nTorshin                        | 173       \nEurobanco                      | 165       \nC+                             | 160       \nanti-austerity                 | 154       \nepv                            | 154       \nexbarón                        | 139       \n°C                             | 138       \n\nPosibles mejoras:\n- 0 palabras podrían encontrarse si pasamos todo a minúsculas.\n('de', ['0.0547', '0.0112', '0.1910', '0.0308', '0.0414', '0.0303', '-0.0879', '-0.0527', '0.0158', '-0.0857', '-0.1319', '-0.2033', '-0.0272', '0.0047', '0.1056', '0.0051', '-0.0890', '-0.3369', '-0.0298', '-0.0366', '0.0356', '0.0275', '-0.2423', '0.1990', '0.0315', '-0.1087', '0.4592', '0.0319', '-0.1572', '0.0212', '0.0050', '-0.0231', '0.0420', '-0.0044', '0.0468', '-0.0553', '-0.0083', '-0.0063', '0.1856', '-0.0214', '-0.0659', '0.1750', '0.4530', '0.0275', '0.0539', '0.0531', '0.1336', '0.0242', '-0.0144', '-0.0746', '0.0363', '0.0312', '-0.1961', '0.0324', '-0.0214', '-0.1861', '0.0126', '-0.0409', '-0.0502', '-0.0315', '0.0475', '0.0140', '0.0186', '-0.0358', '0.0464', '0.0748', '-0.0725', '-0.0742', '-0.0061', '-0.1204', '0.1031', '0.0004', '-0.0474', '0.0104', '-0.0050', '-0.0299', '-0.1193', '0.0752', '-0.0306', '-0.0641', '0.0199', '0.0514', '-0.0097', '0.0365', '-0.0922', '-0.0050', '0.0570', '0.0035', '0.0050', '0.0065', '-0.0104', '0.0397', '0.0645', '-0.0472', '-0.1203', '-0.0136', '-0.0646', '-0.0336', '0.0714', '0.0133', '0.0022', '0.0405', '0.2487', '0.0284', '0.0249', '0.8094', '0.0139', '0.0048', '0.0428', '0.1678', '-0.0743', '0.0255', '0.0638', '-0.0347', '-0.2788', '0.0175', '0.0020', '-0.0387', '-0.1584', '-0.2613', '0.0080', '0.0564', '0.2348', '0.3009', '0.0929', '0.0055', '-0.1004', '0.1165', '-0.0227', '-0.0534', '-0.0325', '-0.0241', '0.0459', '0.0433', '-0.0347', '-0.1191', '0.3836', '-0.0173', '-0.2535', '0.0074', '0.0235', '-0.0881', '-0.0664', '0.1243', '0.2499', '-0.0357', '-0.0205', '-0.0745', '0.0239', '0.0652', '-0.0115', '0.0816', '0.0002', '-0.0418', '-0.1433', '-0.0372', '-0.0685', '-0.0520', '-0.0018', '0.0349', '-0.0383', '-0.0614', '-0.0854', '0.0707', '-0.0208', '-0.0434', '-0.0310', '0.0074', '0.0128', '-0.0993', '-0.0318', '0.4781', '-0.0668', '-0.0312', '-0.0598', '0.1149', '0.0285', '0.0188', '-0.1513', '-0.0644', '-0.1176', '0.0489', '-0.0453', '0.0292', '-0.0649', '-0.0072', '-0.0007', '-0.0094', '0.1152', '-0.0207', '0.0004', '0.0051', '0.1314', '-0.0321', '0.0333', '0.1747', '0.0007', '0.0381', '0.1591', '-0.0554', '-0.4267', '0.6133', '-0.0514', '-0.0043', '0.0855', '-0.0302', '0.0238', '-0.9718', '-0.2726', '-0.0041', '-0.0668', '-0.0025', '0.1160', '0.0379', '-0.0397', '-0.0282', '0.0017', '-0.0897', '-0.0307', '-0.0185', '-0.0188', '0.0402', '-0.0008', '-0.0338', '0.0432', '-0.0307', '0.0375', '-0.0275', '-0.0018', '-0.0193', '0.0384', '0.0702', '0.0171', '0.0184', '-0.0811', '-0.0166', '0.0001', '0.0333', '-0.0595', '0.0371', '-0.0354', '0.0632', '0.0800', '0.1206', '0.0761', '-0.0316', '-0.0390', '0.0192', '-0.0189', '-0.1130', '-0.1025', '0.0149', '-0.0110', '0.0139', '0.0297', '0.0826', '-0.1342', '0.0431', '0.0478', '0.0135', '-0.0570', '0.0075', '0.0321', '0.0400', '0.0484', '-0.0171', '0.0113', '0.0116', '0.0316', '0.0058', '-0.0685', '-0.0155', '0.0407', '0.0049', '-0.0285', '0.1053', '0.0218', '0.0058', '0.0422', '0.2032', '0.0114', '-0.1253', '0.0104', '0.0767', '0.0702', '0.0206', '-0.0226', '0.0441', '-0.2581', '0.1713', '0.0518', '0.1919', '0.1226', '0.0554', '0.0461', '-0.0299', '-0.0148', '0.0066', '-0.0021', '-0.0230'])\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nimport os\nfrom tqdm import tqdm\nimport json\n\ndef decode_sequence_to_text(id_sequence, vocab, oov_id_to_word):\n    \"\"\"\n    Decodifica una secuencia de IDs a texto.\n    \n    Args:\n        id_sequence: List o tensor de IDs\n        vocab: Vocabulary object\n        oov_id_to_word: Dict[int, str] - Mapeo de IDs extendidos a palabras OOV\n    \n    Returns:\n        List[str] - Palabras decodificadas\n    \"\"\"\n    if torch.is_tensor(id_sequence):\n        id_sequence = id_sequence.cpu().tolist()\n    \n    V_base = len(vocab.word_to_id)\n    decoded_words = []\n    \n    for id in id_sequence:\n        id = int(id)\n        \n        # Tokens especiales\n        if id == vocab.word2id(vocab.pad_token):\n            continue  # Ignorar padding\n        elif id == vocab.word2id(vocab.start_decoding):\n            continue  # Ignorar START\n        elif id == vocab.word2id(vocab.end_decoding):\n            break  # Terminar en END\n        # IDs base del vocabulario\n        elif id < V_base:\n            decoded_words.append(vocab.id2word(id))\n        # IDs extendidos (OOV copiado)\n        elif id in oov_id_to_word:\n            decoded_words.append(oov_id_to_word[id])\n        else:\n            decoded_words.append(vocab.unk_token)\n    \n    return decoded_words\n\n\ndef create_oov_id_to_word_map(oov_words, V_base):\n    \"\"\"\n    Crea el mapeo ID a Palabra OOV.\n    \n    Args:\n        oov_words: List[str] - Palabras OOV del ejemplo\n        V_base: int - Tamaño del vocabulario base\n    \n    Returns:\n        Dict[int, str] - Mapeo de ID extendido a palabra OOV\n    \"\"\"\n    oov_id_to_word = {}\n    oov_id = V_base\n    \n    for word in oov_words:\n        if word == '':  # Ignorar padding\n            continue\n        oov_id_to_word[oov_id] = word\n        oov_id += 1\n    \n    return oov_id_to_word\n\n\nclass Generator:\n    \"\"\"\n    Clase para generar resúmenes usando el modelo entrenado.\n    \"\"\"\n    \n    def __init__(self, config, vocab, model_path):\n        \"\"\"\n        Args:\n            config: Config object\n            vocab: Vocabulary object\n            model_path: Ruta al checkpoint del modelo\n        \"\"\"\n        self.config = config\n        self.vocab = vocab\n        self.device = config['device']\n        \n        # Cargar modelo\n        print(f\"Cargando modelo desde {model_path}\")\n        self.model = PointerGeneratorNetwork(config, vocab).to(self.device)\n        \n        checkpoint = torch.load(model_path, map_location=self.device)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.model.eval()\n        \n        print(f\"✓ Modelo cargado (Epoch {checkpoint['epoch']})\")\n        \n        # Beam search\n        self.beam_search = BeamSearch(\n            self.model,\n            vocab,\n            beam_size=config['beam_size'],\n            max_len=config['tgt_len']\n        )\n    \n    def generate(self, test_loader, output_file=None, num_examples=None):\n        \"\"\"\n        Genera resúmenes para el dataset de test.\n        \n        Args:\n            test_loader: DataLoader de test\n            output_file: Archivo donde guardar los resúmenes generados\n            num_examples: Número de ejemplos a generar (None = todos)\n        \n        Returns:\n            List[Dict] - Lista con resultados (source, target, generated)\n        \"\"\"\n        results = []\n        V_base = len(self.vocab.word_to_id)\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Generando resúmenes\")\n        print(f\"{'='*60}\")\n        print(f\"Estrategia: {self.config['decoding_strategy']}\")\n        print(f\"Beam size: {self.config['beam_size']}\")\n        print(f\"{'='*60}\\n\")\n        \n        with torch.no_grad():\n            pbar = tqdm(test_loader, desc=\"Generando\", total=len(test_loader))\n            \n            for batch_idx, batch in enumerate(pbar):\n                batch_size = batch['encoder_input'].size(0)\n                \n                for b in range(batch_size):\n                    # Extraer ejemplo individual\n                    single_batch = {\n                        'encoder_input': batch['encoder_input'][b:b+1],\n                        'extended_encoder_input': batch['extended_encoder_input'][b:b+1],\n                        'encoder_length': batch['encoder_length'][b:b+1],\n                        'encoder_mask': batch['encoder_mask'][b:b+1],\n                        'decoder_target': batch['decoder_target'][b:b+1]\n                    }\n                    \n                    oov_words = batch['oov_words'][b]\n                    oov_map = create_oov_id_to_word_map(oov_words, V_base)\n                    \n                    # Generar resumen\n                    if self.config['decoding_strategy'] == 'beam_search':\n                        hypothesis = self.beam_search.search(single_batch)\n                        generated_ids = hypothesis.tokens[1:]  # Quitar START\n                    else:  # greedy\n                        generated_ids = self.model.decode_greedy(single_batch, max_len=self.config['tgt_len'])\n                        generated_ids = generated_ids[0].cpu().tolist()\n                    \n                    # Decodificar a texto\n                    source_ids = single_batch['extended_encoder_input'][0].cpu().tolist()\n                    target_ids = single_batch['decoder_target'][0].cpu().tolist()\n                    \n                    source_text = decode_sequence_to_text(source_ids, self.vocab, oov_map)\n                    target_text = decode_sequence_to_text(target_ids, self.vocab, oov_map)\n                    generated_text = decode_sequence_to_text(generated_ids, self.vocab, oov_map)\n                    \n                    result = {\n                        'source': ' '.join(source_text),\n                        'target': ' '.join(target_text),\n                        'generated': ' '.join(generated_text)\n                    }\n                    \n                    results.append(result)\n                    \n                    if num_examples and len(results) >= num_examples:\n                        break\n                \n                if num_examples and len(results) >= num_examples:\n                    break\n            \n            pbar.close()\n        \n        print(f\"\\n✓ Generados {len(results)} resúmenes\")\n        \n        # Guardar resultados\n        if output_file:\n            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n            \n            with open(output_file, 'w', encoding='utf-8') as f:\n                json.dump(results, f, ensure_ascii=False, indent=2)\n            \n            print(f\"✓ Resultados guardados en {output_file}\")\n            \n            # También guardar en formato legible\n            txt_file = output_file.replace('.json', '.txt')\n            with open(txt_file, 'w', encoding='utf-8') as f:\n                for i, result in enumerate(results):\n                    f.write(f\"{'='*60}\\n\")\n                    f.write(f\"Ejemplo {i+1}\\n\")\n                    f.write(f\"{'='*60}\\n\")\n                    f.write(f\"SOURCE:\\n{result['source']}\\n\\n\")\n                    f.write(f\"TARGET:\\n{result['target']}\\n\\n\")\n                    f.write(f\"GENERATED:\\n{result['generated']}\\n\\n\")\n            \n            print(f\"✓ Resultados legibles en {txt_file}\")\n        \n        return results\n\n\ndef main():\n    \"\"\"\n    Función principal para generar resúmenes.\n    \"\"\"\n    # 1. Cargar vocabulario\n    print(\"Cargando vocabulario...\")\n    vocab = Vocabulary(\n        CREATE_VOCABULARY=False,  # No crear, solo cargar\n        PAD_TOKEN=PAD_TOKEN,\n        UNK_TOKEN=UNK_TOKEN,\n        START_DECODING=START_DECODING,\n        END_DECODING=END_DECODING,\n        MAX_VOCAB_SIZE=MAX_VOCAB_SIZE,\n        CHECKPOINT_VOCABULARY_DIR=CHECKPOINT_VOCABULARY_DIR,\n        DATA_DIR=DATA_DIR,\n        VOCAB_NAME=VOCAB_NAME\n    )\n    vocab.build_vocabulary()\n    print(f\"✓ Vocabulario cargado: {vocab.total_size()} palabras\")\n    \n    # 2. Configurar\n    config = Config(\n        max_vocab_size=vocab.total_size(),\n        src_len=MAX_LEN_SRC,\n        tgt_len=MAX_LEN_TGT,\n        embedding_size=EMBEDDING_SIZE,\n        hidden_size=HIDDEN_SIZE,\n        num_enc_layers=NUM_ENC_LAYERS,\n        num_dec_layers=NUM_DEC_LAYERS,\n        use_gpu=USE_GPU,\n        is_pgen=IS_PGEN,\n        is_coverage=IS_COVERAGE,\n        dropout_ratio=DROPOUT_RATIO,\n        bidirectional=BIDIRECTIONAL,\n        device=DEVICE,\n        decoding_strategy=DECODING_STRATEGY,\n        beam_size=BEAM_SIZE,\n        gpu_id=GPU_ID\n    )\n    \n    # 3. Dataset de test\n    print(\"\\nCargando dataset de test...\")\n    test_dataset = PGNDataset(\n        vocab=vocab,\n        MAX_LEN_SRC=config['src_len'],\n        MAX_LEN_TGT=config['tgt_len'],\n        data_dir=DATA_DIR,\n        split='test'\n    )\n    \n    print(f\"✓ Test: {len(test_dataset)} ejemplos\")\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=1,  # Procesar de uno en uno para beam search\n        shuffle=True,\n        collate_fn=pgn_collate_fn,\n        num_workers=0\n    )\n    \n    # 4. Ruta del modelo\n    # Usar el mejor modelo por defecto\n    model_path = os.path.join(CHECKPOINT_DIR, 'checkpoint_best2.pt')\n    \n    # Si no existe, buscar el último\n    if not os.path.exists(model_path):\n        model_path = os.path.join(CHECKPOINT_DIR, 'checkpoint_last2.pt')\n    \n    if not os.path.exists(model_path):\n        print(f\"⚠ No se encontró ningún checkpoint en {CHECKPOINT_DIR}\")\n        return\n    \n    # 5. Generar\n    generator = Generator(config, vocab, model_path)\n    \n    # Generar todos los ejemplos (o especificar un número)\n    output_file = os.path.join(GENERATED_TEXT_DIR, 'test_results.json')\n    \n    results = generator.generate(\n        test_loader,\n        output_file=output_file,\n        num_examples=10  # None = todos, o especificar un número\n    )\n    \n    # Mostrar algunos ejemplos\n    print(f\"\\n{'='*60}\")\n    print(\"Ejemplos generados:\")\n    print(f\"{'='*60}\\n\")\n    \n    for i, result in enumerate(results[:20]):\n        print(f\"Ejemplo {i+1}:\")\n        print(f\"SRC: {result['source']}\")\n        print(f\"TARGET: {result['target']}\")\n        print(f\"GENERATED: {result['generated']}\")\n        print()\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T10:44:22.273881Z","iopub.execute_input":"2026-01-02T10:44:22.275740Z","iopub.status.idle":"2026-01-02T10:44:49.902788Z","shell.execute_reply.started":"2026-01-02T10:44:22.275681Z","shell.execute_reply":"2026-01-02T10:44:49.901175Z"}},"outputs":[{"name":"stdout","text":"Cargando vocabulario...\nDescargando modelo de spaCy español (Large)...\nCollecting es-core-news-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/12.9 MB 46.1 MB/s eta 0:00:00\nInstalling collected packages: es-core-news-sm\nSuccessfully installed es-core-news-sm-3.8.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('es_core_news_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\nsaved/working/Vocabulary.json\n Vocabulario cargado desde: saved/working/Vocabulary.json\n Tamaño total: 50000\n Tokens especiales: 4\n Tokens regulares: 49996\n✓ Vocabulario cargado: 50000 palabras\n\nCargando dataset de test...\n✓ Usando archivos TOKENIZADOS para test (Carga optimizada en Kaggle)\n✓ Test: 13920 ejemplos\nCargando modelo desde kaggle/working/saved/checkpoint_best2.pt\n✓ Modelo cargado (Epoch 6)\n\n============================================================\nGenerando resúmenes\n============================================================\nEstrategia: beam_search\nBeam size: 5\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Generando:   0%|          | 9/13920 [00:15<6:35:01,  1.70s/it]","output_type":"stream"},{"name":"stdout","text":"\n✓ Generados 10 resúmenes\n✓ Resultados guardados en kaggle/working/generated/test_results.json\n✓ Resultados legibles en kaggle/working/generated/test_results.txt\n\n============================================================\nEjemplos generados:\n============================================================\n\nEjemplo 1:\nSRC: Puntuación 6,5 Arquitectura 7 Decoración 5 Estado de conservación 7 Confortabilidad habitaciones 6 Aseos 5 Ambiente 6 Desayuno 8 Atención 9 Tranquilidad 7 Instalaciones 6 Merche y José Luis Castillo se anticiparon en 2002 al boom del enoturismo en La Rioja con la apertura de este hotelito-bodega situado en Ábalos , a unos 15 kilómetros de Haro . Sus conocimientos en viticultura y enología , por no mencionar los platos que acompañan estas catas , los guiaron hasta una casa solariega del siglo XVII reconocida por su impecable factura barroca que enseguida reformaron y decoraron con acento rústico , ecléctico y colorista . La piedra adusta , acaramelada con maderas y detalles de forja , era el reclamo necesario para la seriedad que impone un viaje por la arquitectura histórica de la comarca , mencionada en la Crónica albeldense , a los pies de la sierra de Toloño . Su fachada sirve de pantalla cinematográfica a la terraza que se esparce por la plaza de Fermín Gurbindo en las apacibles vísperas de verano . Todos los espacios comunes se concentran en la planta baja el jardín emparrado , la terraza trasera , los salones , la biblioteca y una mesa con mantel y cuatro llamativos butacones donde se oficia la recepción de los huéspedes . Al costado de la casa se abre una portezuela que accede al aparcamiento , solo para la clientela . ampliar foto La fachada del hotel Villa de Ábalos , en La Rioja . Autodidacta , aunque acreditada con varios cursillos en el Basque Culinary Center , Merche ensancha la experiencia de la estancia con una cocina generosa enfocada en el kilómetro cero , aunque no estrictamente dependiente de la huerta riojana . Antes conviene tomar el aperitivo en el gastrobar Empatía , con mesas en el interior y en el exterior , donde fluyen a tutiplén los pinchos de Merche junto a las explicaciones de José Luis acerca de sus vinos Empatía blanco viura , malvasía y garnacha blanca y Empatía tinto tempranillo , garnacha , elaborados en un viñedo familiar que está próximo a cumplir los 70 años , bajo el asesoramiento de Tom Puyaubert . Edificio arriba se distribuyen las 12 habitaciones en dos plantas accesibles por medio de una escalera de forja secundada por exposiciones temporales de pinturas . Suficientemente amplias y cómodas para las exigencias de un caserón histórico cuyo afinamiento en la insonorización se hace imposible . Las traseras se distraen con el verde del jardín , y las altas , con la sierra de la Demanda . Las delanteras miran con sus atisbos barrocos a la sierra de Cantabria y , en primer plano , a la histórica bodega de la Real Divisa , cuya fundación en 1367 la acredita entre las más antiguas de Europa . Villa de Ábalos Categoría oficial 3 estrellas 3 estrellas Dirección plaza de Fermín Gurbindo , 2 . Ábalos La Rioja plaza de Fermín Gurbindo , 2 .\nTARGET: Una casa solariega del siglo XVII cerca de Haro para una escapada [UNK]\nGENERATED: Villa de Ábalos , un hotel familiar en La Rioja con la apertura de este hotelito-bodega situado en Ábalos\n\nEjemplo 2:\nSRC: Nunca tuvo miedo Benito Zambrano Lebrija , 54 años , de no volver a dirigir un largometraje . Jamás , pero cuando atravieso uno de esos momentos larguísimos que vivo entre película y película , pienso ¿ Y yo , en realidad , a qué me dedico ? ¿ Soy de verdad director ? , confiesa . Esa duda quedará apartada hoy cuando esta noche se proyecte Intemperie , el cuarto largometraje de ficción del cineasta sevillano , como película de inauguración de la Seminci , el festival de cine de Valladolid , que comienza así su 64ª edición . Intemperie adapta la novela homónima de Jesús Carrasco , un libro que bebe tanto de Miguel Delibes como de Cormac McCarthy para contar la huida de un niño de un pueblo mísero de la España seca y profunda de la posguerra , arrasada por la violencia y el poder . En su persecución salen el capataz Luis Callejo de la zona y sus hombres , y el crío Jaime López solo encontrará ayuda en un solitario cabrero Luis Tosar . La película , tras su periplo festivalero , llega a los cines comerciales el 22 de noviembre . Zambrano se encontró en plena crisis económica española con que las historias que deseaba contar no eran las que priorizaban las televisiones privadas , los grandes productores españoles de cine . Algún proyecto era de época , y eso lo encarecía . En cambio , mis tres largos han ido siempre bien en taquilla , así que tan mal no lo hago , ríe al finalizar la frase . Tanto Solas 1999 como Habana Blues 2005 y La voz dormida 2011 efectivamente conectaron con los espectadores . Por suerte , apareció Intemperie , y se estrena justo cuando estoy con un guion , Pan de limón con semillas de amapola , adaptación de la novela de la escritora Cristina Campos , cuya preproducción arranca ahora . Los dioses se han apiadado de mí , asegura , y remata Yo siempre he estado trabajando , con la obsesión de generando ideas y proyectos . Ya veremos qué pasará después . ampliar foto Benito Zambrano , en el rodaje , con Luis Tosar . A Zambrano el guion le llegó con el ejercicio difícil ya hecho la labor de expurgar en la novela de Carrasco , de decantar el material fílmico de un férreo ejercicio de contundencia , ya lo habían realizado los hermanos Pablo -actual figura del teatro español- y Daniel Remón . A mí me dan la novela en seco y no hubiera sabido por dónde meterle mano , confiesa el cineasta . Ellos ya habían sabido extraer la película que había dentro de la novela .\nTARGET: La adaptación de la novela de Jesús Carrasco dirigida por Benito Zambrano ahonda en la fiereza de la España rural de la posguerra\nGENERATED: El cineasta sevillano adapta la novela homónima de Jesús Carrasco , un libro que bebe tanto de Miguel Delibes\n\nEjemplo 3:\nSRC: Las acusaciones basadas en pruebas falseadas son una práctica policial corriente en Rusia desde hace muchos años , según un reciente informe del grupo de defensa de derechos humanos . El falseamiento de las pruebas por parte de los órganos policiales se utiliza para apartar de la vida pública a personas que de un modo u otro por sus actividades de investigación periodística , de defensa de derechos humanos , de oposición política o rivalidad empresarial afectan los intereses de los grupos en el poder en distintos ámbitos . El informe titulado ¿ Por qué los ciudadanos de Rusia piden asilo en Europa ? explica que la práctica de falsificar expedientes penales se convirtió en algo sistemático durante la segunda guerra de Chechenia 1999-2003 , cuando , con el fin de limpiar Moscú de ciudadanos de origen checheno , se organizó una campaña para acusarlos de posesión de narcóticos . En el uso de las pruebas amañadas , la posesión o el tráfico de drogas , severamente castigados en Rusia , se une a los falsos cargos por terrorismo , pedofilia o corrupción . Con amplios indicios de ser víctima de pseudoprocesos , han sido condenados el jefe de Memorial de Chechenia , Oyub Titiev que un tribunal moscovita decidió ayer liberar anticipadamente por haber cumplido gran parte de su condena , el cineasta ucraniano Oleg Sentsov , el activista Ruslán Kutaév , entre muchos otros . A este contingente se le puede sumar ahora el periodista de investigación Iván Golunov , de la publicación digital Meduza , que fue detenido el 6 de junio y acusado de dedicarse al tráfico de drogas . Tanto él como sus responsables en Meduza consideran que está siendo castigado por su trabajo y por las investigaciones a las que se estaba dedicando últimamente . Entre los temas en los que ha trabajado el periodista está el negocio funerario en Moscú y asuntos inmobiliarios relacionados con la alcaldía . El caso de Golunov no es uno más . Las alarmas de peligro inminente para la profesión de informador se han activado y los periodistas rusos han salido a la calle , a defender a su colega y a sí mismos , en manifestaciones de protesta sin precedentes . Todos son conscientes de que ellos mismos pueden convertirse en víctimas mañana si permiten hoy el triunfo de un expediente penal amañado o irregular . A la solidaridad se le une el sentido de justicia . Las instituciones internacionales refuerzan el carácter simbólico del caso con su apoyo a Golunov . El caso Golunov es la gota que ha colmado el vaso de la paciencia de los profesionales de la información en Rusia , amordazados , humillados y heridos en su autoestima por un sistema que impone cotidianamente múltiples prohibiciones y castigos . Abogados e informadores están atentos .\nTARGET: Los informadores sufren un intolerable acoso de los servicios de seguridad\nGENERATED: Las acusaciones basadas en pruebas falseadas son una práctica policial corriente en Rusia desde hace muchos años\n\nEjemplo 4:\nSRC: Nunca el rock duro vendió tantos millones de discos en tan poco tiempo . Surgió en la primera mitad de los años ochenta y con el empuje de una efervescente MTV que programaba los vídeos de estos grupos a todas horas hizo millonarios en unos meses a unos veinteañeros que , con alguna excepción , venían de las clases media y baja . Su empeño fue llevar al extremo el consumo de drogas y competir por ver quién acumulaba más relaciones sexuales . Porque fue un movimiento básicamente de hombres heterosexuales dispuestos a vestirse de la forma más estrafalaria posible . Es el hair metal por aquello de las voluminosas melenas cardadas y explotó básicamente en Estados Unidos , para luego expandirse por muchas más zonas . Incluso en España tuvimos nuestros dignos representantes , aunque algunos años más tarde como siempre , por otra parte Sangre Azul , Sobredosis , Tarzen Su objetivo era hacer del rock duro algo sexi . Y lo consiguieron . Pero por encima de todo este exagerado estilismo , este movimiento generó grandes canciones . Aún hoy , más de tres décadas después , esos temas suenan poderosos y plenamente reivindicables . Dos críticos musicales han seleccionado las 10 mejores bandas del género , todas al mismo nivel , sin querer establecer un ranking La banda Dokken presumiendo de cardados en Chicago en 1985 . Foto Getty Dokken Quiénes son . La suma de un elegante vocalista Don Dokken y un guitarrista pirotécnico George Lynch . Su momento sexo , drogas y rock and roll . En marzo de 1988 , mientras se dirigían en una limusina al estadio de Wembley Londres para telonear a AC DC , un desquiciado Lynch agarró por el pelo al cantante y empezó a pegarle . En una entrevista de 2015 , el perfeccionista Don Dokken reveló Los otros tres tenían un vínculo , y eran las drogas . Se metían una raya e iban a su bola . Yo no he consumido cocaína en mi vida . Su temón . De su álbum Under lock and key 1985 se vendieron un millón de ejemplares en Estados Unidos . Pero el anterior , Tooth and nail 1984 , contenía el single Into the fire , epítome de su sonido tiene parte de balada , los arpegios a la moda , un puente con unos coros excelentes , un riff de fantasía en el estribillo y un solo de guitarra que pone el vello como las tachuelas de una muñequera . Dónde están ahora . Como buen producto ochentero , es significativo que se separaran en 1989 . El guitarrista formó Lynch Mob , vehículo para su lucimiento , y Dokken reformó la banda en 1993 , que , con diferentes músicos , sigue en activo . Lynch y el cantante aparcaron su odio mutuo y grabaron en 2018 el tema It s just another day .\nTARGET: Debajo de toda la [UNK] había música de mucha calidad . [UNK] Ocurrió en los ochenta , duró pocos años , hizo millonaria a mucha gente y los protagonistas llevaron al extremo el lema sexo , drogas y rock and roll\nGENERATED: Dos críticos musicales han seleccionado las 10 mejores bandas del género , todas al mismo nivel\n\nEjemplo 5:\nSRC: Ya es aciaga casualidad que después de acusar a Pedro Sánchez de sembrar sectarismo Albert Rivera fuera hospitalizado por una gastroenteritis aguda . La coincidencia promocionó en Twitter una de sus recurrentes discusiones entre quienes bromearon y quienes lo vieron improcedente por tratarse de la salud personal . Debates éticos al margen , aceptemos que si el líder de Ciudadanos lo dijo , sus razones tendrá . Y éstas , fruto de su percepción o de su constatación , son tan subjetivas como lícitas . Le asiste además la misma libertad de expresión que a cualquiera de los catalanes que , recordando el nacimiento tanto de la formación como de su liderazgo , no se extrañan que de aquellos polvos iniciáticos surjan los lodos esparcidos por la política española . Por lo acontecido desde aquella etapa hasta nuestros días , la autoridad moral de Rivera para recriminar a los demás atisbos de sectarismo es tan poco consistente como que el resto de líderes proyecte hacia él . A nuestros representantes difícilmente se les puede conceder hoy el plus de referencia ética para censurar nada a nadie . Todos arrastran su parte alícuota de responsabilidad que , añadida a la que nos corresponde a la ciudadanía , ayuda a entender por qué hemos caído tan bajo . Algo está pasando cuando todos coinciden en la alineación por la ingobernabilidad y el bloqueo y mantienen una claque que se lo aplaude . Lo indican las encuestas . Igual le pasa al independentismo y sus grupos de agitación y propaganda cuando se plantan ante las sedes de los partidos que tenían por propios para recriminarles pactos inadecuados con las circunstancias que les hicieron creer que concurrirían . Pero el destino es cruel y la hora de la verdad siempre llega con un espejo en la mano . Hoy la política se hace más a través de Twitter que de charlas presenciales y propuestas parlamentarias Así , el panorama nos muestra a la derecha contra la izquierda pero de manera evidente contra sí misma , a la izquierda contra la derecha pero especialmente contra sus propios postulados y al secesionismo contra el mundo empezando por el propio . Nada se sustenta ya ni en conceptos ideológicos , ni en paradigmas existenciales ni en otras legítimas aspiraciones que no sean las de optar o mantener las respectivas cuotas de poder y engaño . Por mucho que lo nieguen o intenten disimularlo . Tensaron tanto la cuerda , se obligaron tanto a mantener el pulso que ahora ni siquiera son capaces de ceder ante sus propias contradicciones . No es no , líneas rojas o sí o sí son algunos de los eslóganes convertidos en tendencias de difícil revisión que esclavizan a quienes las crean y obligan a quienes se las creen . Y de ahí la gran irresponsabilidad compartida . Unos , los políticos , por no querer asumir las consecuencias públicas que provocan sus posiciones privadas .\nTARGET: Hubo un tiempo en el que el buen político era aquel que siguiendo la norma de Keynes cambiaba de posición de acuerdo al cambio de las circunstancias . [UNK] Y a eso se le llamaba responsabilidad .\nGENERATED: A nuestros representantes difícilmente se les puede conceder hoy el plus de referencia ética para censurar nada a nadie\n\nEjemplo 6:\nSRC: Un reciente estudio , recogido por el Centro Nacional para la Información en Biotecnología NCBI de Estados Unidos encargado de recopilar artículos científicos referentes en biomedicina , biotecnología , bioquímica , genética y genómica publicados en PubMed , revela que las piezas dentales de leche contienen células madre que , por haber estado menos expuestas a daños medioambientales , pueden ser de gran ayuda para regenerar otras partes dañadas del cuerpo . En los últimos años , han sido varias las investigaciones que se han llevado a cabo sobre la posible potencialidad que tienen las células madre dentales en el desarrollo de terapias para combatir futuras enfermedades . Una circunstancia que ha favorecido la aparición de bancos de dientes privados a los que las familias acuden para conservar las piezas dentarias de sus hijos con la esperanza de que puedan serles de utilidad para el tratamiento de patologías en años venideros . Según envejecemos , la cantidad de células madre adultas que uno retiene en los órganos va disminuyendo bien porque esas células van muriendo , se van utilizando para ir regenerando tejidos u órganos , o van perdiendo capacidad o funcionalidad regenerativa . Ander Izeta , biólogo , responsable del Grupo de Ingeniería Tisular del Instituto Biodonostia y secretario de la Sociedad Española de Terapia Génica y Celular , explica que esto ocurre de manera generalizada en todo el organismo y en la pulpa dental ocurre de igual manera , así que si puedes utilizar un diente de leche de un niño siempre va a ser una fuente de células madre más joven , con mejores capacidades que el diente de una persona de 50 u 80 años . Esto es uno de los problemas por los que a veces nos cuesta utilizarlas , porque la gran mayoría de patologías se desarrollan en personas en edad avanzada y , en algunos casos , cuando uno quiere hacer una aproximación autóloga para utilizar las células del propio paciente , estas pueden estar ya envejecidas o dañadas . Por eso , la mayor capacidad terapéutica de las células madre que contienen los dientes está en las edades más tempranas de la población . Francisco Javier Rodríguez Lozano , miembro de la Red de Terapia Celular TERCEL del Instituto de Salud Carlos III , profesor titular de Odontología de la Universidad de Murcia y miembro de la Unidad de Trasplante y Terapia Celular del Hospital Clínico Universitario Virgen de la Arrixaca e investigador del IMIB-Arrixaca Murcia , afirma que los terceros molares muelas del juicio son los últimos dientes que aparecen en la boca en torno a los 18 años , con lo que se consideran los más jóvenes , de ahí su mayor potencial . No obstante , otras células procedentes de dientes de leche , de edades hasta los 10 años aproximadamente , representan una fuente de mayor capacidad , dada la temprana edad del paciente .\nTARGET: Un estudio revela que estas piezas dentales pueden ser de gran ayuda para regenerar otras partes dañadas del cuerpo\nGENERATED: En los últimos años , han sido varias las investigaciones que se han llevado a cabo sobre la posible potencialidad que tienen las células madre dentales\n\nEjemplo 7:\nSRC: Iñigo Errejón ha propuesto este lunes que Podemos dé la investidura al presidente en funciones Pedro Sánchez con la condición de seguir negociando después . Errejón ha sugerido una solución de mínimos entre el PSOE y Unidas Podemos dado que ambas formaciones llegan a la ronda de consultas con el Rey sin haber alcanzado un acuerdo para la gobernabilidad . El líder de Más Madrid y exnúmero dos de Pablo Iglesias plantea que Podemos y el PSOE se pongan de acuerdo para salvar la investidura sin que esté condicionada a la construcción de un Gobierno de coalición , pero con el compromiso de seguir negociando después . Investidura , y seguir hablando , ha propuesto esta mañana en una entrevista en Los Desayunos de TVE . El planteamiento de Errejón consistiría en que Unidas Podemos admita votar a favor de Pedro Sánchez sin conformar ya un Ejecutivo de coalición con los socialistas , pero que esa posibilidad esté abierta a negociación después de que Sánchez sea investido exitosamente . Así se ganaría tiempo para llegar a una solución de estabilidad para el país . Si Unidas Podemos y el PSOE no se pusieran después de acuerdo , siempre se podría entonces volver a repetir las elecciones , ha considerado . La idea del dirigente de Más Madrid recuerda a la que han planteado algunos barones del PSOE , como el presidente valenciano , Ximo Puig , o el de Castilla- La Mancha , Emiliano García Page , que proponen una especie de coalición en diferido un acuerdo de investidura revisable en un tiempo como un año , o dos , y entonces estudiar si los de Iglesias podrían entrar en el Consejo de Ministros .\nTARGET: El ex número dos de Pablo Iglesias defiende una solución de mínimos entre ambos partidos\nGENERATED: La idea del dirigente de Más Madrid y exnúmero dos de Pablo Iglesias plantea que Podemos y el PSOE se pongan de acuerdo para salvar la investidura\n\nEjemplo 8:\nSRC: Pocas horas antes de la incorporación de Más Madrid al 10-N , ocurrió algo insólito en Córdoba Manuela Carmena salía al escenario del Teatro Góngora para dialogar sobre la vida después de la política , y fue recibida con una ovación más propia de La Scala , al grito abrumador de presidenta , presidenta , presidenta ! Como si acabara efectivamente de triunfar con una Traviata memorable , pero sin haberlo hecho . La propia Carmena recordaba que no había sido capaz de ganar . En todo caso , no es seguro que la pasión carmenista sea extrapolable a Más País , y se entiende por eso que Errejón tratara de incorporarla al cartel . Aunque Carmena apenas diga una frase con particular sustancia política cuestión distinta es preguntarse si algún otro líder sí lo hace , habla un lenguaje que le conecta a un cierto público . Desde luego ella nunca diría aquello de la hegemonía se mueve en la tensión entre el núcleo irradiador y la seducción de los sectores aliados laterales . Hay un espacio para Más País , pero la capacidad de Errejón para liderarlo es una de las incógnitas de su movimiento . Hay otras incógnitas , como sucede con las apariciones tempestivas caso de Vox un año atrás en Andalucía sin la fiabilidad de una serie demoscópica . Con todo , parece difícil salir de la foto fija en la suma final de los bloques izquierda derecha de modo que vaya a deshacerse la aritmética de la gobernabilidad con una ecuación inesperada . Pero hay algo lógico si el impacto castiga sobre todo a Podemos , que ya experimentaba una tendencia a la baja en las encuestas , la aparición de otro líder dispuesto a aprobar un acuerdo programático haría más difícil a Pablo Iglesias condicionar la investidura a tener ministerios . Y , de hecho , Errejón lanza mensajes implacablemente notorios contra la posición de Podemos , al comprometerse a que cada escaño suyo sirva al objetivo de un Gobierno progresista . Uno de sus lugartenientes añadía Si alguien no tiene el talante necesario para llegar a acuerdos que se eche a un lado . Más País Menos Podemos . Sí que parece haber un espacio para Errejón . Y tanto más con la reacción de Podemos acentuando su perfil duro , sobre todo con sus brindis al independentismo en un nuevo ciclo incendiario lo de Colau y Asens cruza algunas líneas rojas avalando el insomnio de Sánchez y a la vez el desplazamiento de este hacia el centro con el mensaje de la estabilidad . Por lo demás , sea cual sea la proporción en que Más País arañe votos a Podemos y PSOE , ese espacio existe . Y si solo concurre en las provincias que reparten más de siete escaños , como parece , no tiene por qué cumplirse la idea de que la fragmentación penaliza . Incluso puede ser al revés .\nTARGET: Errejón además puede ser una cierta esperanza del [UNK] , ese nicho en el que se veía muy cuajado el desencanto tras el fiasco [UNK]\nGENERATED: Si alguien no tiene el talante necesario para llegar a acuerdos que se eche a un lado\n\nEjemplo 9:\nSRC: Tráiler de Los miserables Stéphen es un agente nuevo de la brigada policial contra la delincuencia del suburbio de Montfermeil , en la región de Seine-Saint-Denis , en París . Francia acaba de ganar el Mundial de Fútbol , y el día a día en ese barrio no es nada fácil . Así arranca Los miserables -el título es un guiño a la novela de Víctor Hugo , ya que se desarrolla en la misma zona- , de Ladj Ly , fotógrafo y cineasta de ese mismo distrito , que ya en 2016 codirigió junto a Stéphane de Freitas el documental A viva voz , que ya se acercaba a la problemática realidad de un grupo de jóvenes de Saint-Denis . Ly muestra el público dos conflictivas jornadas en las que la llegada de un circo a Saint-Denis provoca varios choques entre la policía , los muy distintos líderes de la comunidad y los jóvenes del barrio . Premio del Jurado en Cannes y candidata francesa a los Oscar , Los miserables se estrena en España el 22 de noviembre y aquí mostramos en exclusiva su tráiler .\nTARGET: El debut en la dirección de Ladj Ly indaga en la crudeza del día a día en los suburbios parisienses\nGENERATED: Tráiler de Los miserables Stéphen es un guiño a la novela de Víctor Hugo\n\nEjemplo 10:\nSRC: Imagen de la atracción clausurada en San José de la Rinconada Sevilla . europa press atlas Corrían las dos de la madrugada del pasado sábado cuando el pánico se adueñó de la feria de San José de la Rinconada Sevilla . El asiento de la atracción conocida como La Superolla , en el que se agarran los usuarios , se desprendió por motivos que se están investigando provocando que varios de los jóvenes que se habían montado salieran disparados . Un total de 28 personas , la mayoría menores de edad , resultaron heridos y nueve de ellos tuvieron que ser trasladados a varios hospitales de la capital andaluza . Tres de ellos siguen ingresados , la más grave , una chica de 13 años , está fuera de peligro . El Ayuntamiento de la localidad ha abierto una investigación para esclarecer lo sucedido y ha anunciado que se personará en la causa contra los dueños de la atracción y la Asociación de Feriantes de Andalucía que es la entidad que explota el parque de atracciones de San José . El consistorio ha confirmado que la instalación en cuestión había pasado todos los controles de seguridad previos y que tenía en regla todos los permisos pertinentes y que cumplía con los requisitos legales . La atracción había superado satisfactoriamente los controles de seguridad previos al inicio de la feria , así como su documentación técnica cumplía con todos los requisitos legales . Son los órganos competentes los que en estos momentos están realizando las investigaciones y averiguaciones oportunas que conduzcan a la explicación de los hechos sucedidos , señala el comunicado del consistorio . Desde primera hora de esta mañana , la Policía Judicial está estudiando lo que pudo suceder . Una de las hipótesis que se barajan es que la falta de mantenimiento por obsolescencia del aparato hubiera provocado el desprendimiento del asiento . Los técnicos municipales revisan que la documentación esté en regla , pero no evalúan los sistemas de montaje , que es una de las causas que podría haber provocado la avería . En el momento en que tuvo lugar el accidente , el resto de las atracciones del recinto ferial cerraron . No obstante , la evolución favorable de los heridos ha determinado a los responsables a permitir que el parque reanudara su funcionamiento a lo largo de esta noche y del domingo , último día de festejos . Aunque la mayoría de los accidentados ha recibido el alta , la angustia que les generó el incidente continúa y muchos de ellos continúan muy afectados . El Ayuntamiento también ha confirmado que pondrá sus servicios jurídicos a disposición de las familias por su quieren iniciar actuaciones legales . Los accidentes en recintos feriales de los pueblos no suelen ser frecuentes , pero son más habituales que en los parques de atracciones .\nTARGET: La feria de la localidad sevillana reanuda su actividad este sábado al evolucionar favorablemente los 28 heridos\nGENERATED: Un total de 28 personas fueron heridos y nueve de ellos tuvieron que ser trasladados a varios hospitales de la capital andaluza\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Evaluar","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nimport os\nfrom tqdm import tqdm\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom rouge import Rouge\nfrom nltk.translate.meteor_score import meteor_score\nimport nltk\n\n\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    nltk.download('punkt', quiet=True)\ntry:\n    nltk.data.find('corpora/wordnet')\nexcept LookupError:\n    nltk.download('wordnet', quiet=True)\n\n\ndef decode_sequence_to_text(id_sequence, vocab, oov_id_to_word):\n    \"\"\"Decodifica una secuencia de IDs a texto.\"\"\"\n    if torch.is_tensor(id_sequence):\n        id_sequence = id_sequence.cpu().tolist()\n    \n    V_base = len(vocab.word_to_id)\n    decoded_words = []\n    \n    for id in id_sequence:\n        id = int(id)\n        \n        if id == vocab.word2id(vocab.pad_token):\n            continue\n        elif id == vocab.word2id(vocab.start_decoding):\n            continue\n        elif id == vocab.word2id(vocab.end_decoding):\n            break\n        elif id < V_base:\n            decoded_words.append(vocab.id2word(id))\n        elif id in oov_id_to_word:\n            decoded_words.append(oov_id_to_word[id])\n        else:\n            decoded_words.append(vocab.unk_token)\n    \n    return decoded_words\n\n\ndef create_oov_id_to_word_map(oov_words, V_base):\n    \"\"\"Crea el mapeo ID a Palabra OOV.\"\"\"\n    oov_id_to_word = {}\n    oov_id = V_base\n    \n    for word in oov_words:\n        if word == '':\n            continue\n        oov_id_to_word[oov_id] = word\n        oov_id += 1\n    \n    return oov_id_to_word\n\n\ndef calculate_rouge_scores(reference, candidate):\n    \"\"\"\n    Calcula ROUGE scores usando la librería rouge.\n    \n    Args:\n        reference: str - Resumen de referencia\n        candidate: str - Resumen generado\n        \n    Returns:\n        Dict con ROUGE-1, ROUGE-2 y ROUGE-L scores\n    \"\"\"\n    if not reference.strip() or not candidate.strip():\n        return {\n            'rouge-1': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n            'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n            'rouge-l': {'f': 0.0, 'p': 0.0, 'r': 0.0}\n        }\n    \n    rouge = Rouge()\n    try:\n        scores = rouge.get_scores(candidate, reference)[0]\n        return scores\n    except Exception as e:\n        print(f\"⚠ Error calculando ROUGE: {e}\")\n        return {\n            'rouge-1': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n            'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n            'rouge-l': {'f': 0.0, 'p': 0.0, 'r': 0.0}\n        }\n\n\ndef calculate_meteor(reference, candidate):\n    \"\"\"\n    Calcula METEOR score usando NLTK.\n    \n    Args:\n        reference: str - Resumen de referencia\n        candidate: str - Resumen generado\n        \n    Returns:\n        float - METEOR score\n    \"\"\"\n    if not reference.strip() or not candidate.strip():\n        return 0.0\n    \n    try:\n        # METEOR requiere lista de tokens\n        reference_tokens = reference.split()\n        candidate_tokens = candidate.split()\n        \n        # METEOR espera una lista de referencias\n        score = meteor_score([reference_tokens], candidate_tokens)\n        return score\n    except Exception as e:\n        print(f\"⚠ Error calculando METEOR: {e}\")\n        return 0.0\n\n\nclass Evaluator:\n        \n    \"\"\"\n    Clase para evaluar el modelo en el dataset de test.\n    \"\"\"\n    \n    def __init__(self, config, vocab, model_path):\n        \"\"\"\n        Args:\n            config: Config object\n            vocab: Vocabulary object\n            model_path: Ruta al checkpoint del modelo\n        \"\"\"\n        self.config = config\n        self.vocab = vocab\n        self.device = config['device']\n        \n        # Cargar modelo\n        print(f\"Cargando modelo desde {model_path}\")\n        self.model = PointerGeneratorNetwork(config, vocab).to(self.device)\n        \n        checkpoint = torch.load(model_path, map_location=self.device)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.model.eval()\n        \n        print(f\"✓ Modelo cargado (Epoch {checkpoint['epoch']})\")\n        print(f\"✓ Best Val Loss: {checkpoint.get('best_val_loss', 'N/A')}\")\n        \n        # Beam search\n        self.beam_search = BeamSearch(\n            self.model,\n            vocab,\n            beam_size=config['beam_size'],\n            max_len=config['tgt_len']\n        )\n    def _copy_rate(self, candidate_tokens, source_tokens):\n            \"\"\"Porcentaje de palabras del resumen generado que aparecen en el source.\"\"\"\n            if not candidate_tokens or not source_tokens:\n                return 0.0\n            source_set = set(source_tokens)\n            copy_count = sum(1 for w in candidate_tokens if w in source_set)\n            return copy_count / len(candidate_tokens)\n\n    def _ngram_overlap(self, candidate_tokens, source_tokens, n=2):\n            \"\"\"Porcentaje de n-gramas del resumen generado que aparecen en el source.\"\"\"\n            if len(candidate_tokens) < n or len(source_tokens) < n:\n                return 0.0\n            def ngrams(tokens, n):\n                return set(tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1))\n            cand_ngrams = ngrams(candidate_tokens, n)\n            src_ngrams = ngrams(source_tokens, n)\n            if not cand_ngrams:\n                return 0.0\n            overlap = len(cand_ngrams & src_ngrams)\n            return overlap / len(cand_ngrams)\n    def evaluate(self, test_loader, num_examples=None):\n        \"\"\"\n        Evalúa el modelo en el dataset de test.\n        \n        Args:\n            test_loader: DataLoader de test\n            num_examples: Número de ejemplos a evaluar (None = todos)\n            \n        Returns:\n            Dict con métricas y resultados\n        \"\"\"\n        V_base = len(self.vocab.word_to_id)\n        \n        # Métricas acumuladas\n        rouge1_scores = []\n        rouge2_scores = []\n        rougeL_scores = []\n        meteor_scores = []\n        copy_rates = []\n        bigram_overlaps = []\n        p_gen_avgs = [] # Promedio de p_gen por ejemplo\n        \n        test_loss = 0.0\n        num_batches = 0\n        \n        results = []\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Evaluando modelo en Test Set\")\n        print(f\"{'='*60}\")\n        print(f\"Estrategia: {self.config['decoding_strategy']}\")\n        print(f\"Beam size: {self.config['beam_size']}\")\n        print(f\"{'='*60}\\n\")\n        \n        with torch.no_grad():\n            pbar = tqdm(test_loader, desc=\"Evaluando\", total=len(test_loader))\n            \n            for batch_idx, batch in enumerate(pbar):\n                if batch is None:\n                    continue\n                \n                # Calcular loss\n                outputs = self.model(batch, is_training=True)\n                test_loss += outputs['loss'].item()\n                num_batches += 1\n                \n                batch_size = batch['encoder_input'].size(0)\n                \n                for b in range(batch_size):\n                    # Extraer ejemplo individual\n                    single_batch = {\n                        'encoder_input': batch['encoder_input'][b:b+1],\n                        'extended_encoder_input': batch['extended_encoder_input'][b:b+1],\n                        'encoder_length': batch['encoder_length'][b:b+1],\n                        'encoder_mask': batch['encoder_mask'][b:b+1],\n                        'decoder_target': batch['decoder_target'][b:b+1]\n                    }\n                    \n                    oov_words = batch['oov_words'][b]\n                    oov_map = create_oov_id_to_word_map(oov_words, V_base)\n                    \n                    # Generar resumen\n                    if self.config['decoding_strategy'] == 'beam_search':\n                        hypothesis = self.beam_search.search(single_batch)\n                        generated_ids = hypothesis.tokens[1:]  # Quitar START\n                        # Extraer p_gens de la hipótesis (lista de tensores (1,1))\n                        p_gens_list = hypothesis.p_gens\n                        if p_gens_list:\n                            p_gens_tensor = torch.cat(p_gens_list).squeeze() # (tgt_len,)\n                        else:\n                            p_gens_tensor = torch.tensor([])\n                            \n                    else:  # greedy\n                        generated_ids, p_gens_tensor = self.model.decode_greedy(single_batch, max_len=self.config['tgt_len'])\n                        generated_ids = generated_ids[0].cpu().tolist()\n                        p_gens_tensor = p_gens_tensor[0].squeeze(-1).cpu() # (max_len,)\n                    \n                    # Calcular promedio de p_gen para este ejemplo\n                    if p_gens_tensor.numel() > 0:\n                        avg_p_gen_example = p_gens_tensor.mean().item()\n                    else:\n                        avg_p_gen_example = 0.0\n                        \n                    p_gen_avgs.append(avg_p_gen_example)\n                    \n                    # Decodificar a texto\n                    target_ids = single_batch['decoder_target'][0].cpu().tolist()\n                    \n                    reference = decode_sequence_to_text(target_ids, self.vocab, oov_map)\n                    candidate = decode_sequence_to_text(generated_ids, self.vocab, oov_map)\n                    \n                    # Convertir a string\n                    reference_text = ' '.join(reference)\n                    candidate_text = ' '.join(candidate)\n                    \n                    # Calcular ROUGE usando la librería\n                    rouge_scores = calculate_rouge_scores(reference_text, candidate_text)\n                    # Calcular METEOR\n                    meteor = calculate_meteor(reference_text, candidate_text)\n                    rouge1_scores.append(rouge_scores['rouge-1']['f'])\n                    rouge2_scores.append(rouge_scores['rouge-2']['f'])\n                    rougeL_scores.append(rouge_scores['rouge-l']['f'])\n                    meteor_scores.append(meteor)\n                    # Calcular tasa de copia y bigram overlap\n                    src_ids = single_batch['encoder_input'][0].cpu().tolist()\n                    source_tokens = decode_sequence_to_text(src_ids, self.vocab, oov_map)\n                    copy_rate = self._copy_rate(candidate, source_tokens)\n                    bigram_overlap = self._ngram_overlap(candidate, source_tokens, n=2)\n\n                    copy_rates.append(copy_rate)\n                    bigram_overlaps.append(bigram_overlap)\n\n                    # Guardar resultado\n                    result = {\n                        'reference': reference_text,\n                        'candidate': candidate_text,\n                        'rouge1_f1': rouge_scores['rouge-1']['f'],\n                        'rouge2_f1': rouge_scores['rouge-2']['f'],\n                        'rougeL_f1': rouge_scores['rouge-l']['f'],\n                        'meteor': meteor,\n                        'copy_rate': copy_rate,\n                        'bigram_overlap': bigram_overlap,\n                        'avg_p_gen': avg_p_gen_example,\n                        'avg_p_copy': 1.0 - avg_p_gen_example\n                    }\n                    results.append(result)\n                    \n                    # Update progress bar\n                    pbar.set_postfix({\n                        'loss': f\"{outputs['loss'].item():.4f}\",\n                        'R1': f\"{np.mean(rouge1_scores):.4f}\",\n                        'R2': f\"{np.mean(rouge2_scores):.4f}\",\n                        'RL': f\"{np.mean(rougeL_scores):.4f}\",\n                        'M': f\"{np.mean(meteor_scores):.4f}\",\n                        'Copy': f\"{np.mean(copy_rates):.2f}\",\n                        'BiOv': f\"{np.mean(bigram_overlaps):.2f}\",\n                        'P_Gen': f\"{np.mean(p_gen_avgs):.2f}\"\n                    })\n                    \n                    if num_examples and len(results) >= num_examples:\n                        break\n                \n                if num_examples and len(results) >= num_examples:\n                    break\n            \n            pbar.close()\n        \n        # Calcular promedios\n        avg_test_loss = test_loss / num_batches if num_batches > 0 else 0.0\n        avg_rouge1 = np.mean(rouge1_scores) if rouge1_scores else 0.0\n        avg_rouge2 = np.mean(rouge2_scores) if rouge2_scores else 0.0\n        avg_rougeL = np.mean(rougeL_scores) if rougeL_scores else 0.0\n        avg_meteor = np.mean(meteor_scores) if meteor_scores else 0.0\n        \n        avg_copy_rate = np.mean(copy_rates) if copy_rates else 0.0\n        avg_bigram_overlap = np.mean(bigram_overlaps) if bigram_overlaps else 0.0\n        avg_p_gen = np.mean(p_gen_avgs) if p_gen_avgs else 0.0\n        \n        metrics = {\n            'test_loss': avg_test_loss,\n            'rouge1_f1': avg_rouge1,\n            'rouge2_f1': avg_rouge2,\n            'rougeL_f1': avg_rougeL,\n            'meteor': avg_meteor,\n            'copy_rate': avg_copy_rate,\n            'bigram_overlap': avg_bigram_overlap,\n            'avg_p_gen': avg_p_gen,\n            'avg_p_copy': 1.0 - avg_p_gen,\n            'num_examples': len(results)\n        }\n        \n        return metrics, results\n    \n    def save_results(self, metrics, results, output_dir):\n        \"\"\"\n        Guarda los resultados de la evaluación.\n        \n        Args:\n            metrics: Dict con métricas\n            results: Lista de resultados por ejemplo\n            output_dir: Directorio donde guardar\n        \"\"\"\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Guardar métricas\n        metrics_path = os.path.join(output_dir, 'test_metrics.json')\n        with open(metrics_path, 'w', encoding='utf-8') as f:\n            json.dump(metrics, f, indent=2)\n        print(f\"\\n✓ Métricas guardadas en {metrics_path}\")\n        \n        # Guardar resultados completos\n        results_path = os.path.join(output_dir, 'test_results.json')\n        with open(results_path, 'w', encoding='utf-8') as f:\n            json.dump(results, f, ensure_ascii=False, indent=2)\n        print(f\"✓ Resultados guardados en {results_path}\")\n        \n        # Guardar formato legible\n        txt_path = os.path.join(output_dir, 'test_results.txt')\n        with open(txt_path, 'w', encoding='utf-8') as f:\n            f.write(f\"{'='*60}\\n\")\n            f.write(f\"MÉTRICAS DE EVALUACIÓN\\n\")\n            f.write(f\"{'='*60}\\n\")\n            f.write(f\"Test Loss: {metrics['test_loss']:.4f}\\n\")\n            f.write(f\"ROUGE-1 F1: {metrics['rouge1_f1']:.4f}\\n\")\n            f.write(f\"ROUGE-2 F1: {metrics['rouge2_f1']:.4f}\\n\")\n            f.write(f\"ROUGE-L F1: {metrics['rougeL_f1']:.4f}\\n\")\n            f.write(f\"METEOR: {metrics['meteor']:.4f}\\n\")\n            f.write(f\"Copy Rate: {metrics['copy_rate']:.4f}\\n\")\n            f.write(f\"Bigram Overlap: {metrics['bigram_overlap']:.4f}\\n\")\n            f.write(f\"Avg P_Gen: {metrics['avg_p_gen']:.4f}\\n\")\n            f.write(f\"Avg P_Copy: {metrics['avg_p_copy']:.4f}\\n\")\n            f.write(f\"Ejemplos evaluados: {metrics['num_examples']}\\n\")\n            f.write(f\"{'='*60}\\n\\n\")\n            \n            for i, result in enumerate(results[:10]):  # Primeros 10 ejemplos\n                f.write(f\"{'='*60}\\n\")\n                f.write(f\"Ejemplo {i+1}\\n\")\n                f.write(f\"{'='*60}\\n\")\n                f.write(f\"REFERENCE:\\n{result['reference']}\\n\\n\")\n                f.write(f\"GENERATED:\\n{result['candidate']}\\n\\n\")\n                f.write(f\"ROUGE-1: {result['rouge1_f1']:.4f} | \")\n                f.write(f\"ROUGE-2: {result['rouge2_f1']:.4f} | \")\n                f.write(f\"ROUGE-L: {result['rougeL_f1']:.4f} | \")\n                f.write(f\"METEOR: {result['meteor']:.4f} | \")\n                f.write(f\"Copy Rate: {result['copy_rate']:.4f} | \")\n                f.write(f\"Bigram Overlap: {result['bigram_overlap']:.4f} | \")\n                f.write(f\"P_Gen: {result['avg_p_gen']:.4f} | \")\n                f.write(f\"P_Copy: {result['avg_p_copy']:.4f}\\n\\n\")\n        \n        print(f\"✓ Resultados legibles en {txt_path}\")\n\n\ndef plot_training_history(output_dir, checkpoint_path):\n    \"\"\"\n    Grafica la historia de entrenamiento desde el checkpoint del modelo.\n    \n    Args:\n        output_dir: Directorio donde guardar las gráficas\n        checkpoint_path: Ruta al checkpoint del modelo\n    \"\"\"\n    if not os.path.exists(checkpoint_path):\n        print(f\"⚠ No se encontró el checkpoint {checkpoint_path}\")\n        return\n    \n    print(f\"\\nCargando historial de entrenamiento desde {checkpoint_path}...\")\n    \n    # Cargar checkpoint\n    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n    \n    # Extraer historial del checkpoint\n    history = checkpoint.get('train_history', None)\n    \n    if not history or not history.get('epoch'):\n        print(\"⚠ El historial está vacío o no existe en el checkpoint\")\n        return\n    \n    # Extraer datos\n    epochs = history['epoch']\n    train_losses = history['train_loss']\n    val_losses = history['val_loss']\n    \n    # Crear figura con 2 subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Plot 1: Train y Val Loss\n    ax1.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2)\n    ax1.plot(epochs, val_losses, 'r-', label='Val Loss', linewidth=2)\n    ax1.set_xlabel('Epoch', fontsize=12)\n    ax1.set_ylabel('Loss', fontsize=12)\n    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n    ax1.legend(fontsize=10)\n    ax1.grid(True, alpha=0.3)\n    \n    # Plot 2: Vocab Loss y Coverage Loss\n    vocab_losses = history.get('vocab_loss', [])\n    coverage_losses = history.get('coverage_loss', [])\n    \n    if vocab_losses and coverage_losses:\n        ax2.plot(epochs, vocab_losses, 'g-', label='Vocab Loss', linewidth=2)\n        ax2.plot(epochs, coverage_losses, 'r-', label='Coverage Loss', linewidth=2)\n        ax2.set_xlabel('Epoch', fontsize=12)\n        ax2.set_ylabel('Loss', fontsize=12)\n        ax2.set_title('Vocab Loss vs Coverage Loss', fontsize=14, fontweight='bold')\n        ax2.legend(fontsize=10)\n        ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    # Guardar figura\n    os.makedirs(output_dir, exist_ok=True)\n    plot_path = os.path.join(output_dir, 'training_curves.png')\n    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n    print(f\"✓ Gráfica guardada en {plot_path}\")\n    \n    # Mostrar estadísticas\n    print(f\"\\n{'='*60}\")\n    print(f\"ESTADÍSTICAS DE ENTRENAMIENTO\")\n    print(f\"{'='*60}\")\n    print(f\"Épocas completadas: {len(epochs)}\")\n    print(f\"Mejor Train Loss: {min(train_losses):.4f} (Epoch {epochs[train_losses.index(min(train_losses))]})\")\n    print(f\"Mejor Val Loss: {min(val_losses):.4f} (Epoch {epochs[val_losses.index(min(val_losses))]})\")\n    print(f\"Última Train Loss: {train_losses[-1]:.4f}\")\n    print(f\"Última Val Loss: {val_losses[-1]:.4f}\")\n    \n    if vocab_losses and coverage_losses:\n        print(f\"Última Vocab Loss: {vocab_losses[-1]:.4f}\")\n        print(f\"Última Coverage Loss: {coverage_losses[-1]:.4f}\")\n    \n    print(f\"{'='*60}\\n\")\n    \n    plt.close()\n\n\ndef main():\n    \"\"\"\n    Función principal para evaluar el modelo.\n    \"\"\"\n    # 1. Cargar vocabulario\n    print(\"Cargando vocabulario...\")\n    vocab = Vocabulary(\n        CREATE_VOCABULARY=False,\n        PAD_TOKEN=PAD_TOKEN,\n        UNK_TOKEN=UNK_TOKEN,\n        START_DECODING=START_DECODING,\n        END_DECODING=END_DECODING,\n        MAX_VOCAB_SIZE=MAX_VOCAB_SIZE,\n        CHECKPOINT_VOCABULARY_DIR=CHECKPOINT_VOCABULARY_DIR,\n        DATA_DIR=DATA_DIR,\n        VOCAB_NAME=VOCAB_NAME\n    )\n    vocab.build_vocabulary()\n    print(f\"✓ Vocabulario cargado: {vocab.total_size()} palabras\")\n    \n    # 2. Configurar\n    config = Config(\n        max_vocab_size=vocab.total_size(),\n        src_len=MAX_LEN_SRC,\n        tgt_len=MAX_LEN_TGT,\n        embedding_size=EMBEDDING_SIZE,\n        hidden_size=HIDDEN_SIZE,\n        num_enc_layers=NUM_ENC_LAYERS,\n        num_dec_layers=NUM_DEC_LAYERS,\n        use_gpu=USE_GPU,\n        is_pgen=IS_PGEN,\n        is_coverage=IS_COVERAGE,\n        coverage_lambda=COV_LOSS_LAMBDA,\n        dropout_ratio=DROPOUT_RATIO,\n        bidirectional=BIDIRECTIONAL,\n        device=DEVICE,\n        decoding_strategy=DECODING_STRATEGY,\n        beam_size=BEAM_SIZE,\n        gpu_id=GPU_ID\n    )\n    \n    # 3. Dataset de test\n    print(\"\\nCargando dataset de test...\")\n    test_dataset = PGNDataset(\n        vocab=vocab,\n        MAX_LEN_SRC=config['src_len'],\n        MAX_LEN_TGT=config['tgt_len'],\n        data_dir=DATA_DIR,\n        split='test'\n    )\n    \n    print(f\"✓ Test: {len(test_dataset)} ejemplos\")\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=1,\n        shuffle=True,\n        collate_fn=pgn_collate_fn,\n        num_workers=0\n    )\n    \n    # 4. Ruta del modelo\n    model_path = os.path.join(CHECKPOINT_DIR, 'checkpoint_best2.pt')\n    \n    if not os.path.exists(model_path):\n        model_path = os.path.join(CHECKPOINT_DIR, 'checkpoint_last1.pt')\n    \n    if not os.path.exists(model_path):\n        print(f\"⚠ No se encontró ningún checkpoint en {CHECKPOINT_DIR}\")\n        return\n    \n    output_dir = GENERATED_TEXT_DIR\n    \n    # 5. Graficar pérdida de entrenamiento (desde el checkpoint)\n    plot_training_history(output_dir, model_path)\n    \n    # 6. Evaluar\n    evaluator = Evaluator(config, vocab, model_path)\n    \n    metrics, results = evaluator.evaluate(\n        test_loader,\n        num_examples=30  # None = todos\n    )\n    \n    # 7. Mostrar resultados\n    print(f\"\\n{'='*60}\")\n    print(f\"RESULTADOS DE EVALUACIÓN\")\n    print(f\"{'='*60}\")\n    print(f\"Test Loss:   {metrics['test_loss']:.4f}\")\n    print(f\"ROUGE-1 F1:  {metrics['rouge1_f1']:.4f}\")\n    print(f\"ROUGE-2 F1:  {metrics['rouge2_f1']:.4f}\")\n    print(f\"ROUGE-L F1:  {metrics['rougeL_f1']:.4f}\")\n    print(f\"METEOR:      {metrics['meteor']:.4f}\")\n    print(f\"Ejemplos:    {metrics['num_examples']}\")\n    print(f\"{'='*60}\\n\")\n    \n    # 8. Guardar resultados\n    evaluator.save_results(metrics, results, output_dir)\n    \n    # 9. Mostrar algunos ejemplos\n    print(f\"\\n{'='*60}\")\n    print(\"EJEMPLOS DE RESÚMENES GENERADOS\")\n    print(f\"{'='*60}\\n\")\n    \n    for i, result in enumerate(results[:3]):\n        print(f\"Ejemplo {i+1}:\")\n        print(f\"REFERENCE: {result['reference']}...\")\n        print(f\"GENERATED: {result['candidate']}...\")\n        print(f\"ROUGE-1: {result['rouge1_f1']:.4f} | \"\n              f\"ROUGE-2: {result['rouge2_f1']:.4f} | \"\n              f\"ROUGE-L: {result['rougeL_f1']:.4f} | \"\n              f\"METEOR: {result['meteor']:.4f} | \"\n              f\"P_Gen: {result['avg_p_gen']:.4f}\")\n        print()\n\n\nif __name__ == \"__main__\":\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T10:49:52.044570Z","iopub.execute_input":"2026-01-02T10:49:52.045061Z","iopub.status.idle":"2026-01-02T10:50:52.034841Z","shell.execute_reply.started":"2026-01-02T10:49:52.045032Z","shell.execute_reply":"2026-01-02T10:50:52.033658Z"}},"outputs":[{"name":"stdout","text":"Cargando vocabulario...\nsaved/working/Vocabulary.json\n Vocabulario cargado desde: saved/working/Vocabulary.json\n Tamaño total: 50000\n Tokens especiales: 4\n Tokens regulares: 49996\n✓ Vocabulario cargado: 50000 palabras\n\nCargando dataset de test...\n✓ Usando archivos TOKENIZADOS para test (Carga optimizada en Kaggle)\n✓ Test: 13920 ejemplos\n\nCargando historial de entrenamiento desde kaggle/working/saved/checkpoint_best2.pt...\n✓ Gráfica guardada en kaggle/working/generated/training_curves.png\n\n============================================================\nESTADÍSTICAS DE ENTRENAMIENTO\n============================================================\nÉpocas completadas: 6\nMejor Train Loss: 3.0379 (Epoch 6)\nMejor Val Loss: 3.2034 (Epoch 6)\nÚltima Train Loss: 3.0379\nÚltima Val Loss: 3.2034\nÚltima Vocab Loss: 3.0191\nÚltima Coverage Loss: 0.0188\n============================================================\n\nCargando modelo desde kaggle/working/saved/checkpoint_best2.pt\n✓ Modelo cargado (Epoch 6)\n✓ Best Val Loss: 3.2033928868211348\n\n============================================================\nEvaluando modelo en Test Set\n============================================================\nEstrategia: beam_search\nBeam size: 5\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Evaluando:   0%|          | 29/13920 [00:56<7:29:07,  1.94s/it, loss=3.3668, R1=0.2413, R2=0.0930, RL=0.2020, M=0.1981, Copy=0.95, BiOv=0.90, P_Gen=0.25]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nRESULTADOS DE EVALUACIÓN\n============================================================\nTest Loss:   2.7415\nROUGE-1 F1:  0.2413\nROUGE-2 F1:  0.0930\nROUGE-L F1:  0.2020\nMETEOR:      0.1981\nEjemplos:    30\n============================================================\n\n\n✓ Métricas guardadas en kaggle/working/generated/test_metrics.json\n✓ Resultados guardados en kaggle/working/generated/test_results.json\n✓ Resultados legibles en kaggle/working/generated/test_results.txt\n\n============================================================\nEJEMPLOS DE RESÚMENES GENERADOS\n============================================================\n\nEjemplo 1:\nREFERENCE: Si existían sospechas sobre la erosión del orden liberal en el mundo , lo sucedido esta semana en Bruselas permite abandonar cualquier atisbo de duda...\nGENERATED: El Consejo es otra cosa los ultras están más coordinados que nunca...\nROUGE-1: 0.0000 | ROUGE-2: 0.0000 | ROUGE-L: 0.0000 | METEOR: 0.0422 | P_Gen: 0.1981\n\nEjemplo 2:\nREFERENCE: Urge el debate de un programa nacional para incorporar la bici a un sistema [UNK] de transporte...\nGENERATED: La bici se ha convertido en un signo de distinción , compromiso y modernidad...\nROUGE-1: 0.2069 | ROUGE-2: 0.0000 | ROUGE-L: 0.2069 | METEOR: 0.1890 | P_Gen: 0.2913\n\nEjemplo 3:\nREFERENCE: El cantautor Fran Fernández ha financiado su nuevo disco mediante micromecenazgo . [UNK] Antes de llevar a cabo la campaña ya tenía comprometidos 50 conciertos en Latinoamérica...\nGENERATED: Fran Fernández es el último trabajo de Fran Fernández , un cantautor que concibe la música como un viaje...\nROUGE-1: 0.2381 | ROUGE-2: 0.0476 | ROUGE-L: 0.1905 | METEOR: 0.1627 | P_Gen: 0.4444\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install rouge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T10:48:42.238395Z","iopub.execute_input":"2026-01-02T10:48:42.238707Z","iopub.status.idle":"2026-01-02T10:48:46.735410Z","shell.execute_reply.started":"2026-01-02T10:48:42.238686Z","shell.execute_reply":"2026-01-02T10:48:46.734140Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# En tu notebook de Kaggle\nimport os\nfrom IPython.display import FileLink\n\n# Comprimir archivos específicos\nimport zipfile\n\n# Listar archivos en /kaggle/working/\narchivos = os.listdir('/kaggle/working/kaggle/working/saved/')\nprint(\"Archivos disponibles:\", archivos)\n\n# Crear zip con outputs importantes\nwith zipfile.ZipFile('/kaggle/working/outputs.zip', 'w') as zipf:\n    for file in archivos:\n        if file.endswith('checkpoint_last1.pt'):\n            zipf.write(f'/kaggle/working//kaggle/working/saved/{file}', file)\n\n# Crear enlace para descarga\ndisplay(FileLink('outputs.zip'))","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2026-01-02T07:37:43.034612Z","iopub.status.idle":"2026-01-02T07:37:43.034875Z","shell.execute_reply.started":"2026-01-02T07:37:43.034762Z","shell.execute_reply":"2026-01-02T07:37:43.034777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}